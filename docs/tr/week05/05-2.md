---
lang-ref: ch.05-2
title: Optimizasyon Yöntemleri 2
lecturer: Aaron Defazio
authors: Guido Petri, Haoyue Ping, Chinmay Singhal, Divya Juneja
date: 24 Feb 2020
translation date: 20 Jul 2020
translator: melikenurm
---


## [Adaptif yöntemler](https://www.youtube.com/watch?v=--NZb480zlg&t=2675s)

Momentumlu SGD şu anda birçok makine öğrenmesi problemi için en iyi optimizasyon yöntemidir. Ancak Adaptif Yöntemler adı verilerek yıllar boyunca geliştirilen başka yöntemler de vardır. Bu yöntemler özellikle şartları zayıf belirlenmiş problemler için SGD işe yaramazsa işe yararlar.

SGD formülasyonunda, ağdaki her bir ağırlık aynı öğrenme oranına sahip bir denklem kullanılarak güncellenir (global $\gamma$). Adaptif yöntemlerde ise, *her bir ağırlık için ayrı bir öğrenme oranı uyarlanır*. Her ağırlık için gradyanlardan aldığımız bilgiler bu amaçla kullanılır.

Pratikte sıklıkla kullanılan ağların farklı bölümlerinde farklı yapıları vardır. Örneğin, CNN'in ilk bölümlerinde büyük görüntülerde çok sığ evrişim katmanları daha sonraki bölümlerinde ise küçük görüntülerde çok sayıda kanalın evrişimleri olabilir. Bu işlemlerin her ikisi de çok farklı olduğundan, ağın başlangıcı için iyi çalışan bir öğrenme oranı, ağın sonraki bölümleri için iyi çalışmayabilir. Bu, katmana göre adapte edilen öğrenme oranlarının yararlı olabileceği anlamına gelir.

Ağın ikinci kısmındaki ağırlıklar (aşağıdaki şekil 1'de 4096) doğrudan çıktıyı belirler ve çok güçlü bir etkiye sahiptir. Bu nedenle, bunlar için daha düşük öğrenme oranları gerekir. Aksine ilk kısımdaki ağırlıklar özellikle de rastgele başlatıldığında çıktı üzerindeki bireysel etkileri daha küçük olacaktır.

<center>
<img src="{{site.baseurl}}/images/week05/05-2/5_2_vgg.png" style="zoom:40%"><br>
<b>Şekil 1: </b>VGG16
</center>


### RMSprop

*Karelerin ortalamasının karekökü yayılımı(Root Mean Square Propagation)* nın ana fikri, gradyanın karelerin ortalamasının karekökü ile normalleştirilmesidir.

Aşağıdaki denklem, gradyanın karesinin alınması sırasında vektörün her bir elemanının ayrı ayrı karelerinin alındığını gösterir.

$$
\begin{aligned}
v_{t+1} &= {\alpha}v_t + (1 - \alpha) \nabla f_i(w_t)^2 \\
w_{t+1} &=  w_t - \gamma \frac {\nabla f_i(w_t)}{ \sqrt{v_{t+1}} + \epsilon}
\end{aligned}
$$

burada $\gamma$ küresel öğrenme oranı, $\epsilon$, $\epsilon$ makinesine yakın bir değerdir ($10^{- 7}$ veya $10^{- 8}$ türünden) -- sıfıra bölünme hatalarından kaçınmak için, $v_{t+1}$ ise 2. moment tahminidir.

*Üstel hareketli ortalama(exponential moving average)* (zaman içinde değişebilecek bir miktarın ortalamasını korumak için standart bir yöntem) yoluyla bu gürültülü miktarı tahmin etmek için $v$ değerini güncelliyoruz. Daha fazla bilgi sağladıkları için yeni değerlere daha büyük ağırlıklar vermeliyiz. Bunu yapmanın bir yolu, eski değerlerin ağırlıklarını üstel olarak düşürmektir. $v$ hesaplamasında çok eski olan değerler 0 ile 1 arasında değişen bir $\alpha$ sabiti ile her adımda aşağı doğru ağırlıklandırılır. Bu, eski değerleri artık üstel hareketli ortalamanın önemli bir parçası olmayıncaya kadar azaltır.

Orijinal yöntem merkezi olmayan bir ikinci momentin üstel hareketli ortalamasını tutar yani burada ortalamayı çıkartmayız. *İkinci moment*, gradyanı eleman bazında normalleştirmek için kullanılır, yani gradyanın her elemanı ikinci moment tahmininin kareköküne bölünür. Gradyanın beklenen değeri küçükse, bu işlem gradyanın standart sapmaya bölünmesine benzer.

Paydada küçük bir $\epsilon$ kullanmak fark etmez çünkü $v$ çok küçük olduğunda zaten momentum da çok küçüktür.


### ADAM

ADAM yani *Adaptif Moment Tahmini(Adaptive Moment Estimation)* RMSprop yöntemine momentum ekleyen ve daha yaygın kullanılan bir yöntemdir. Momentum güncellemesi üstel hareketli ortalamaya dönüştürülür ve $\beta$ ile uğraşırken öğrenme oranını değiştirmemiz gerekmez. Tıpkı RMSprop'ta olduğu gibi burada da gradyanın karesinin üstel hareketli ortalamasını alırız.

$$
\begin{aligned}
m_{t+1} &= {\beta}m_t + (1 - \beta) \nabla f_i(w_t) \\
v_{t+1} &= {\alpha}v_t + (1 - \alpha) \nabla f_i(w_t)^2 \\
w_{t+1} &=  w_t - \gamma \frac {m_{t}}{ \sqrt{v_{t+1}} + \epsilon}
\end{aligned}
$$

burada $m_{t+1}$, momentumun üstel hareketli ortalamasıdır.

Başlangıçtaki iterasyonlarda hareketli ortalamayı tarafsız tutmak için kullanılan yanlılık düzeltmesi(bias correction) burada gösterilmemiştir.


### Uygulama tarafı

Sinir ağlarını eğitirken, SGD eğitim sürecinin başlangıcında genellikle yanlış yöne giderken, RMSprop doğru yönde ilerler. Bununla birlikte, RMSprop da klasik SGD gibi gürültüden muzdariptir, yani yerel bir minimum bulunduğunda optimumun etrafında yaptığı sıçramalar kritik sonuçlara yol açar. Tıpkı SGD'ye ivme eklediğimizde olduğu gibi, ADAM ile de aynı iyileştirmeyi elde ederiz. Bu, çözümün gürültülü olmayan iyi bir tahminidir, bu nedenle **ADAM genellikle RMSprop üzerinden önerilir**.

<center>
<img src="{{site.baseurl}}/images/week05/05-2/5_2_comparison.png" style="zoom:45%"><br>
<b>Şekil 2: </b> SGD, RMSprop ve ADAM karşılaştırması
</center><br>

ADAM, dil modellerini kullanmak için bazı ağları eğitmede gereklidir. Sinir ağlarını optimize etmek için genellikle momentumlu SGD veya ADAM tercih edilir. Bununla birlikte, ADAM'ın yayınlardaki teorisi iyi anlaşılmamıştır ve bazı dezavantajları da vardır:

* Çok basit test problemlerinde yöntemin yakınsamadığı görülebilir.
* Genelleme hataları verdiği bilinmektedir. Sinir ağı, eğitim için kullanılan verilerde sıfır kayıp verecek şekilde eğitilmişse daha önce hiç görmediği diğer veri noktalarında sıfır kayıp vermeyecektir. Özellikle görüntü problemlerinde SGD kullandığımızdakinden daha kötü genelleme hataları almamız oldukça yaygındır. ADAM veya yapısındaki faktörler,  örneğin en yakın yerel minimumu bulmasını veya daha az gürültüyü içerebilir.
* ADAM ile 3 tampon bulundurmamız gerekirken, SGD 2 tampon gerektirir. Bu durum birkaç gigabayt boyutundaki bir modeli eğitmediğimiz sürece çok önemli değildir ancak böyle olursa belleğe sığmayabilir.
* 1 yerine 2 momentum parametresi ayarlanmalıdır.

## [Normalizasyon katmanları](https://www.youtube.com/watch?v=--NZb480zlg&t=3907s)

Optimizasyon algoritmalarını iyileştirmek yerine, *normalizasyon katmanları* ağ yapısının kendisini iyileştirir. Bunlar mevcut katmanlar arasındaki ek katmanlardır. Amaçları optimizasyon ve genelleme performanslarını iyileştirmektir.

Sinir ağlarında tipik olarak doğrusal işlemleri doğrusal olmayan işlemlerle değiştiririz. Doğrusal olmayan işlemler aktivasyon fonksiyonları olarak da bilinir, ör.ReLU. Normalizasyon katmanlarını doğrusal katmanlardan önce veya aktivasyon fonksiyonlarından sonra yerleştirebiliriz. En yaygın uygulama aşağıdaki şekilde olduğu gibi doğrusal katmanlar ve aktivasyon fonksiyonları arasına yerleştirmektir.

| <center><img src="{{site.baseurl}}/images/week05/05-2/5_2_norm_layer_a.png" width="200px"/></center> | <center><img src="{{site.baseurl}}/images/week05/05-2/5_2_norm_layer_b.png" width="200px"/></center> | <center><img src="{{site.baseurl}}/images/week05/05-2/5_2_norm_layer_c.png" width="225px"/></center> |
| (a) Normalizasyonu eklemeden önce                              |                (b) Normalizasyonu ekledikten sonra                |                    (c) CNN'lerde bir örnek                    |

<center><b>Şekil 3:</b> Normalleştirme katmanlarının tipik konumları.</center>

Şekil 3(c)'de, evrişim doğrusal katmandır, bunu toplu normalizasyon ve ardından ReLU takip eder.

Şunu unutmayın ki, normalizasyon katmanları üzerinden geçen verileri etkiler, yani ağın gücünü değiştirmez, ağırlıkların düzgün ayarlanmasıyla normalleştirilmemiş bir ağ da normalleştirilmiş bir ağ ile aynı çıkışı verebilir.

### Normalizasyon işlemleri

Normalizasyonun genel gösterimi:

$$
y = \frac{a}{\sigma}(x - \mu) + b
$$

burada $x$ girdi vektörü, $y$ çıktı vektörü, $\mu$, $x$'in tahmini ortalaması, $\sigma$, $x$'in tahmini standart sapması(std) , $a$ öğrenilebilir ölçeklendirme faktörü ve $b$ öğrenilebilir yanlılık(bias) terimidir.

Öğrenilebilir parametreler $a$ ve $b$ olmadan, çıkış vektörü $y$ nin dağılımı sabit ortalama = 0 ve std = 1'e sahip olacaktır. Ölçeklendirme faktörü $a$ ve yanlılık terimi $b$  ağın temsil gücünü sağlar, yani çıkış değerleri herhangi bir aralıkta olabilir. $a$ ve $b$ normalizasyonu tersine çevirmez, çünkü bunlar öğrenilebilir parametrelerdir ve $\mu$ ve $\sigma$ 'dan çok daha kararlıdır.

<center>
<img src="{{site.baseurl}}/images/week05/05-2/5_2_norm_operations.png"/><br>
<b>Şekil 4:</b> Normalizasyon işlemleri.
</center>

Giriş vektörünü normalize etmek için normalizasyon örneklerinin nasıl seçileceğine bağlı birkaç yol vardır. Şekil 4'te $H$ yüksekliğindeki ve $W$ genişliğindeki $N$ görüntüden oluşan bir mini-grup için $C$ kanallı 4 farklı normalleştirme yaklaşımı listelenmektedir:

- *Toplu norm(Batch norm)*: normalizasyon girişin yalnızca bir kanalına uygulanır. Bu ilk önerilen ve en iyi bilinen yaklaşımdır. Daha fazla bilgi için lütfen [ResNet 7'nizi Nasıl Eğitirsiniz: Toplu Norm] (https://myrtle.ai/how-to-train-your-resnet-7-batch-norm/) sayfasını okuyun.
- *Katman normu(Layer norm)*: normalleştirme bir görüntü içindeki tüm kanallara uygulanır.
- *Örnek normu(Instance norm)*: normalleştirme yalnızca bir görüntü ve bir kanal üzerine uygulanır.
- *Grup normu(Group norm)*: normalizasyon bir görüntüye ancak birkaç kanala uygulanır. Örneğin, 0'dan 9'a kadar olan kanallar bir gruptur, 10 ile 19 arası başka bir gruptur vb. Uygulamada grup büyüklüğü hemen hemen her zaman 32'dir. Bu uygulamada iyi bir performansa sahip olduğu ve SGD ile çelişmediği için Aaron Defazio tarafından önerilen yaklaşımdır.

Uygulamada toplu norm ve grup normu bilgisayarlı görme problemleri için iyi çalışırken, katman normu ve örnek normu çoğunlukla dil problemleri için kullanılmaktadır.


### Normalizasyon neden işe yarar?

Normalizasyon pratikte iyi çalışıyor olsa da etkinliğinin ardındaki nedenler hala tartışmalıdır. Başlangıçta normalizasyon "iç ortak değişken kaymasını" azalttığı için önerilmiştir, ancak bazı araştırmacılar deneylerinde bunun yanlış olduğunu kanıtlamıştır. Bununla birlikte normalizasyonun aşağıdaki faktörlerin bir kombinasyonuna sahip olduğu açıktır:

- Normalizasyon katmanlarına sahip ağların optimize edilmesi daha kolaydır ve daha büyük öğrenme oranlarının kullanılmasına izin verir. Normalizasyon sinir ağlarının eğitimini hızlandıran bir optimizasyon etkisine sahiptir.
- Ortalama/std tahminleri, gruplardaki örneklerin rastgele olması nedeniyle gürültülüdür. Bu ekstra "gürültü" bazı durumlarda daha iyi genelleme sağlar. Normalizasyonun bir düzenlileştirme(regularization) etkisi vardır.
- Normalizasyon ağırlıkların başlangıç durumuna bağlı olan duyarlılığı azaltır.

As a result, normalization lets you be more "careless" -- you can combine almost any neural network building blocks together and have a good chance of training it without having to consider how poorly conditioned it might be.

Sonuç olarak, normalizasyon daha "dikkatsiz" olabilmenizi sağlar --  neredeyse tüm sinir ağlarını bloklar halinde bir araya getirebilir ve ne kadar kötü şartlandırılmış olduğunu düşünmek zorunda kalmadan eğitme şansınız olabilir.


### Uygulamada dikkat edilecek hususlar

Geri yayılımın, normalizasyon uygulamasının yanı sıra ortalama ve standart sapma hesaplamasıyla yapıldığına dikkat etmeliyiz: çünkü başka şekilde yapılırsa ağ eğitimi sapacaktır. Geri yayılım hesaplaması oldukça zor ve hataya açıktır ancak PyTorch bunu bizim için otomatik olarak hesaplayabilir, bu da çok işimize yarar. PyTorch'ta bulunan iki normalizasyon katmanı sınıfı aşağıda listelenmiştir:

```python
torch.nn.BatchNorm2d(num_features, ...)
torch.nn.GroupNorm(num_groups, num_channels, ...)
```

Toplu norm, ilk olarak geliştirilen ve en yaygın olarak bilinen yöntemdir. Ancak **Aaron Defazio bunun yerine grup normu kullanılmasını önerir**. Daha kararlı, teorik olarak daha basit ve genellikle daha iyi çalışıyor. Grup boyutunun 32 seçilmesi iyi bir varsayılan değerdir.

Toplu norm ve örnek normu için kullanılan ortalama/standart sapma, ağ her değerlendirildiğinde yeniden hesaplanmak yerine eğitimden sonra sabitlenir. Çünkü normalizasyonu gerçekleştirmek için birden fazla eğitim örneği gereklidir. Grup normu ve katman normu için bu gerekli değildir, çünkü onlarda normalizasyon sadece bir eğitim örneğinin üzerindendir.


## [Optimizasyonun Ölümü](https://www.youtube.com/watch?v=--NZb480zlg&t=4817s)

Sometimes we can barge into a field we know nothing about and improve how they are currently implementing things. One such example is the use of deep neural networks in the field of Magnetic Resonance Imaging (MRI) to accelerate MRI image reconstruction.

Bazen hakkında hiçbir şey bilmediğimiz bir alana girebilir ve şu anda uygulamakta oldukları şeyleri geliştirebiliriz. Böyle bir örnek, Manyetik Rezonans Görüntüleme (MRI) alanında MRI görüntü yapılandırmasını hızlandırmak için derin sinir ağlarının kullanılmasıdır.

<center>
<img src="{{site.baseurl}}/images/week05/05-2/5_2_conv_xkcd.png" style="zoom:60%"><br>
<b>Şekil 5:</b> Bazen gerçekten işe yarıyor!
</center>


### MRI Yapılandırması

In the traditional MRI reconstruction problem, raw data is taken from an MRI machine and an image is reconstructed from it using a simple pipeline/algorithm. MRI machines capture data in a 2-dimensional Fourier domain, one row or one column at a time (every few milliseconds). This raw input is composed of a frequency and a phase channel and the value represents the magnitude of a sine wave with that particular frequency and phase. Simply speaking, it can be thought of as a complex valued image, having a real and an imaginary channel. If we apply an inverse Fourier transform on this input, i.e add together all these sine waves weighted by their values, we can get the original anatomical image.

Geleneksel MRI yapılandırma probleminde, ham veriler bir MRI makinesinden alınır ve basit bir boru hattı/algoritma kullanılarak bir görüntü yeniden oluşturulur. MRI makineleri, verileri her seferinde bir satır veya bir sütun (birkaç milisaniyede bir) olmak üzere 2 boyutlu bir Fourier alanında yakalar. Bu ham girdi, bir frekans ve bir faz kanalından oluşur ve değer, belirli bir frekans ve faza sahip bir sinüs dalgasının büyüklüğü(magnitude) ile temsil edilmektedir. Basitçe söylemek gerekirse gerçek ve hayali birer kanala sahip karmaşık değerli bir görüntü olarak düşünülebilir. Bu girişe bir Ters-Fourier dönüşümü uygularsak, yani değerlerine göre ağırlıklandırılmış tüm bu sinüs dalgalarını toplarsak, orijinal anatomik görüntüyü elde edebiliriz.

<center>
<img src="{{site.baseurl}}/images/week05/05-2/5_2_mri.png" style="zoom:60%"/><br>
<bŞekil 6:</b> MRI yapılandırması
</center><br>

A linear mapping currently exists to go from the Fourier domain to the image domain and it's very efficient, literally taking milliseconds, no matter how big the image is. But the question is, can we do it even faster?

Şu anda Fourier alanından görüntü alanına gitmek için doğrusal bir eşleme vardır, tam anlamıyla milisaniyeler almaktadır ve görüntü ne kadar büyük olursa olsun çok etkilidir. Sorun şu, bunu daha da hızlı yapabilir miyiz?

### Hızlandırılmış MRI

The new problem that needs to be solved is accelerated MRI, where by acceleration we mean making the MRI reconstruction process much faster. We want to run the machines quicker and still be able to produce identical quality images. One way we can do this and the most successful way so far has been to not capture all the columns from the MRI scan. We can skip some columns randomly, though it's useful in practice to capture the middle columns, as they contain a lot of information across the image, but outside them we just capture randomly. The problem is that we can't use our linear mapping anymore to reconstruct the image. The rightmost image in Figure 7 shows the output of a linear mapping applied to the subsampled Fourier space. It's clear that this method doesn't give us very useful outputs, and that there's room to do something a little bit more intelligent.

Çözülmesi gereken yeni sorun hızlandırılmış MRG'dir, burada hızlanma ile MRI rekonstrüksiyon sürecini çok daha hızlı hale getirmeyi kastediyoruz. Makineleri daha hızlı çalıştırmak istiyoruz ve yine de aynı kalitede görüntüler üretebiliyoruz. Bunu yapabilmemizin bir yolu ve şimdiye kadarki en başarılı yol, MRI taramasından tüm sütunları yakalamamaktı. Bazı sütunları rastgele atlayabiliriz, ancak pratikte orta sütunları yakalamak yararlıdır, çünkü görüntüde çok fazla bilgi içerdiklerinden, ancak bunların dışında rastgele yakalarız. Sorun şu ki, görüntüyü yeniden yapılandırmak için artık lineer haritalamamızı kullanamıyoruz. Şekil 7'deki en sağdaki görüntü, alt örneklenmiş Fourier boşluğuna uygulanan doğrusal bir eşlemenin çıktısını göstermektedir. Bu yöntemin bize çok faydalı çıktılar vermediği ve biraz daha akıllıca bir şeyler yapmak için yer olduğu açıktır.

<center>
<img src="{{site.baseurl}}/images/week05/05-2/5_2_acc_mri.png" style="zoom:45%"><br>
<b>Şekil 7:</b> Alt örneklenmiş Fourier-uzayında doğrusal eşleme
</center><br>


### Sıkıştırılmış algılama

One of the biggest breakthroughs in theoretical mathematics for a long time was compressed sensing. A paper by <a href="https://arxiv.org/pdf/math/0503066.pdf">Candes et al.</a> showed that theoretically, we can get a perfect reconstruction from the subsampled Fourier-domain image. In other words, when the signal we are trying to reconstruct is sparse or sparsely structured, then it is possible to perfectly reconstruct it from fewer measurements. But there are some practical requirements for this to work -- we don't need to sample randomly, rather we need to sample incoherently -- though in practice, people just end up sampling randomly. Additionally, it takes the same time to sample a full column or half a column, so in practice we also sample entire columns.

Teorik matematikte uzun zamandır en büyük atılımlardan biri sıkıştırılmış algılama idi. <a href="https://arxiv.org/pdf/math/0503066.pdf"> Candes ve arkadaşlarının </a> bir makalesi, teorik olarak, alt örneklenmiş Fourier alan adı görüntüsünden mükemmel bir yeniden yapılandırma elde edebileceğimizi gösterdi. . Başka bir deyişle, yeniden yapılandırmaya çalıştığımız sinyal seyrek veya seyrek yapılandırılmışsa, daha az ölçümden mükemmel bir şekilde yeniden yapılandırmak mümkündür. Ancak bunun çalışması için bazı pratik gereksinimler vardır - rastgele örneklememiz gerekmez, aksine tutarsız örneklememiz gerekir - pratikte insanlar rastgele örnekleme yaparlar. Ayrıca, tam bir sütunu veya yarım sütunu örneklemek de aynı zamanı alır, bu nedenle pratikte tüm sütunları örnekliyoruz.

Another condition is that we need to have *sparsity* in our image, where by sparsity we mean a lot of zeros or black pixels in the image. The raw input can be represented sparsely if we do a wavelength decomposition, but even this decomposition gives us an approximately sparse and not an exactly sparse image. So, this approach gives us a pretty good but not perfect reconstruction, as we can see in Figure 8. However, if the input were very sparse in the wavelength domain, then we would definitely get a perfect image.

Başka bir koşul, görüntümüzde * sparsity * olması gerektiğidir, burada sparlity ile görüntüde çok sayıda sıfır veya siyah piksel anlamına gelir. Dalga boyu ayrışımı yaparsak ham girdi seyrek olarak temsil edilebilir, ancak bu ayrışma bile bize tam olarak seyrek değil, yaklaşık seyrek bir görüntü verir. Bu nedenle, bu yaklaşım bize Şekil 8'de görebileceğimiz gibi oldukça iyi ama mükemmel olmayan bir yeniden yapılandırma sağlar. Bununla birlikte, giriş dalga boyu alanında çok seyrek olsaydı, kesinlikle mükemmel bir görüntü elde ederiz.

<center>
<img src="{{site.baseurl}}/images/week05/05-2/5_2_comp_sensing.png" style="zoom:50%"><br>
<b>Şekil 8: </b>Sıkıştırılmış algılama
</center><br>

Compressed sensing is based on the theory of optimization. The way we can get this reconstruction is by solving a mini-optimization problem which has an additional regularization term:

Sıkıştırılmış algılama, optimizasyon teorisine dayanır. Bu yeniden yapılandırmayı elde etmenin yolu, ek bir düzenlileştirme terimi olan bir mini optimizasyon problemini çözmektir:

$$
\hat{x} = \arg\min_x \frac{1}{2} \Vert M (\mathcal{F}(x)) - y \Vert^2 + \lambda TV(x)
$$

where $M$ is the mask function that zeros out non-sampled entries, $\mathcal{F}$ is the Fourier transform, $y$ is the observed Fourier-domain data, $\lambda$ is the regularization penalty strength, and $V$ is the regularization function.

$ M $ örneklenmemiş girdileri sıfırlayan maske işlevidir, $ \ mathcal {F} $ Fourier dönüşümüdür, $ y $ gözlemlenen Fourier-etki alanı verileri, $ \ lambda $ düzenleme ceza gücüdür ve $ V $, normalleştirme işlevidir.

The optimization problem must be solved for each time step or each "slice" in an MRI scan, which often takes much longer than the scan itself. This gives us another reason to find something better.

Optimizasyon problemi MRI taramasındaki her zaman adımı veya her "dilim" için çözülmelidir; bu genellikle taramanın kendisinden çok daha uzun sürer. Bu bize daha iyi bir şey bulmamız için başka bir neden daha veriyor.


### Optimizasyon kime lazım?

Instead of solving the little optimization problem at every time step, why not use a big neural network to produce the required solution directly? Our hope is that we can train a neural network with sufficient complexity that it essentially solves the optimization problem in one step and produces an output that is as good as the solution obtained from solving the optimization problem at each time step.

Her zaman adımda küçük optimizasyon problemini çözmek yerine, neden gerekli çözümü doğrudan üretmek için büyük bir sinir ağı kullanmıyorsunuz? Umudumuz, bir sinir ağını optimizasyon problemini temel olarak bir adımda çözecek ve her bir adım adımda optimizasyon probleminin çözülmesinden elde edilen çözüm kadar iyi bir çıktı üretecek yeterli karmaşıklıkla eğitebilmemizdir.

$$
\hat{x} = B(y)
$$

where $B$ is our deep learning model and $y$ is the observed Fourier-domain data.

burada $ B $ bizim derin öğrenme modelimiz ve $ y $ ise gözlemlenen Fourier alan adı verileridir.

15 years ago, this approach was difficult -- but nowadays this is a lot easier to implement. Figure 9 shows the result of a deep learning approach to this problem and we can see that the output is much better than the compressed sensing approach and looks very similar to the actual scan.

15 yıl önce, bu yaklaşım zordu - ancak bugünlerde uygulanması çok daha kolay. Şekil 9, bu soruna derin bir öğrenme yaklaşımının sonucunu göstermektedir ve çıktının sıkıştırılmış algılama yaklaşımından çok daha iyi olduğunu ve gerçek taramaya çok benzediğini görebiliriz.

<center>
<img src="{{site.baseurl}}/images/week05/05-2/5_2_dl_approach.png" style="zoom:60%"><br>
<b>Şekil 9: </b>Derin Öğrenme yaklaşımı
</center><br>

The model used to generate this reconstruction uses an ADAM optimizer, group-norm normalization layers, and a U-Net based convolutional neural network. Such an approach is very close to practical applications and we will hopefully be seeing these accelerated MRI scans happening in clinical practice in a few years' time.
-->

Bu rekonstrüksiyonu oluşturmak için kullanılan model, bir ADAM iyileştirici, grup norm normalleştirme katmanları ve U-Net tabanlı evrişimli sinir ağı kullanır. Böyle bir yaklaşım pratik uygulamalara çok yakındır ve umarım klinik uygulamada bu hızlandırılmış MRI taramalarının birkaç yıl içinde gerçekleştiğini göreceğiz.
-->

