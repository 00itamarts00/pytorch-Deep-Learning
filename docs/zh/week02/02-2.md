---
lang-ref: ch.02-2
title: 为神经网络的模组计算梯度，与反向传播的实用技巧
authors: Micaela Flores, Sheetal Laad, Brina Seidel, Aishwarya Rajan
date: 3 February 2020
translator: Titus Tzeng
---


## [一个反向传播的具体例子还有介绍基础的神经网络模组](https://www.youtube.com/watch?v=d9vdh3b787Y&t=3022s)


### 范例

接下来我们会考虑一个反向传播的例子，并使用图像来辅助。任意的函数 $G(w)$ 输入到损失函数 $C$ 中，这可以用一个图来表示。经由雅可比矩阵的乘法操作，我们能将这个图转换成一个反向计算梯度的图。（注意 Pytorch 和 Tensorflow 已经自动地为使用者完成这件事了，也就是说，向前的图自动的被「倒反」来创造导函数的图形以反向传播梯度。）

<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-1.png" alt="梯度的图像" style="zoom:40%;" /></center>

在这个范例中，右方的绿色图代表梯度的图。跟着图从最上方的节点开始的是：

$$
\frac{\partial C(y,\bar{y})}{\partial w}=1 \cdot \frac{\partial C(y,\bar{y})}{\partial\bar{y}}\cdot\frac{\partial G(x,w)}{\partial w}
$$

从维度来说，$\frac{\partial C(y,\bar{y})}{\partial w}$ 是一个行向量，大小为 $1\times N$，其中 $N$ 是 $w$ 中成员的数量；$\frac{\partial C(y,\bar{y})}{\partial \bar{y}}$ 是个大小 $1\times M$ 的行向量，其中 $M$ 是输出的维度；$\frac{\partial \bar{y}}{\partial w}=\frac{\partial G(x,w)}{\partial w}$ 是个大小 $M\times N$ 的矩阵，其中 $M$ 是 $G$ 输出的数量，而 $N$ 是 $w$ 的维度。

当图的结构不固定而是对应于资料时，情况可能更为复杂。比如，我们可以根据输入向量的长度来选择神经网络的模组。虽然这是可行的，当回圈数量过度增加，处理这个变化的难度会增加。



### 基本的神经网络模组

除了习惯的线性和 ReLU 模组，还有其他预先建立的模组。他们十分有用因为他们为了各自的功能被特别的优化过（而非只是用其他初阶模组拼凑而成）。

- 线性： $Y=W\cdot X$

$$
\begin{aligned}
\frac{dC}{dX} &= W^\top \cdot \frac{dC}{dY} \\
\frac{dC}{dW} &= \frac{dC}{dY} \cdot X^\top
\end{aligned}
$$

- ReLU： $y=(x)^+$

  $$
  \frac{dC}{dX} =
      \begin{cases}
        0 & x<0\\
        \frac{dC}{dY} & \text{otherwise}
      \end{cases}
  $$

- 重复： $Y_1=X$, $Y_2=X$

  - 如同一个「分接线」，两个输出都与输入相同

  - 反向传播时，梯度相加

  - 可以类似的分配成 n 个分支

    $$
    \frac{dC}{dX}=\frac{dC}{dY_1}+\frac{dC}{dY_2}
    $$


- 相加： $Y=X_1+X_2$

  - 当两个变数相加，其中一个若被改变，输出也会以相同幅度改变，即

    $$
    \frac{dC}{dX_1}=\frac{dC}{dY}\cdot1 \quad \text{and}\quad \frac{dC}{dX_2}=\frac{dC}{dY}\cdot1
    $$


- 最大值： $Y=\max(X_1,X_2)$

  - 因为这个函数也可以写作：

$$
Y=\max(X_1,X_2)=\begin{cases}
      X_1 & X_1 > X_2 \\
      X_2 & \text{else}
   \end{cases}
\Rightarrow
\frac{dY}{dX_1}=\begin{cases}
      1 & X_1 > X_2 \\
      0 & \text{else}
   \end{cases}
$$

  - 因此，根据链式法则 

$$
\frac{dC}{dX_1}=\begin{cases}
      \frac{dC}{dY}\cdot1 & X_1 > X_2 \\
      0 & \text{else}
   \end{cases}
$$


## [LogSoftMax vs SoftMax](https://www.youtube.com/watch?v=d9vdh3b787Y&t=3985s)

*SoftMax*，另一个 Pytorch 模组，是一种方便的方式，可以将一组数字转换为 0 到 1 之间的数值，并使它们和为 1。这些数字可以理解为几率分布。因此，它经常用于分类问题。下方等式中的 $y_i$ 是一个向量记录每个类别的几率。

$$
y_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$

然而，使用 softmax 使网络容易面临梯度消失。梯度消失是一个问题，因为它会使得随后的权重无法被神经网络改动，进而停止神经网络进一步训练。Logistic sigmoid 函数，就是单一数值的 Softmax 函数，展现出当 s 很大时，$h(s)$ 是 1，而当 s 很小时，$h(s)$ 是 0。因为 sigmoid 函数在 $h(s) = 0$ 和 $h(s) = 1$ 处是平坦的，其梯度为 0，造成消失的梯度。

<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-2.png" alt="描绘了梯度消失的 sigmoid 函数" style="background-color:#DCDCDC;" /></center>

$$
h(s) = \frac{1}{1 + \exp(-s)}
$$

数学家想到可以用 logsoftmax 来解决 softmax 造成的梯度消失问题。*LogSoftMax* 是 Pytorch 当中的另一个基本模组。正如下方等式所示，*LogSoftMax* 组合了 softmax 和对数。

$$
\log(y_i )= \log\left(\frac{\exp(x_i)}{\Sigma_j \exp(x_j)}\right) = x_i - \log(\Sigma_j \exp(x_j)
$$

下方的等式提供同一个等式的另一种观点。下图显示函数中 $\log(1 + \exp(s))$ 的部份。当 s 非常小，其值为 0，至于 s 很大时，值是 s。如此一来，它不会造成<!-- saturate-->饱和，就避免了梯度消失。

$$
\log\left(\frac{\exp(s)}{\exp(s) + 1}\right)= s - \log(1 + \exp(s))
$$

<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-3.png" width='400px' alt="函数的指数部份" /></center>


## [反向传播的实用技巧](https://www.youtube.com/watch?v=d9vdh3b787Y&t=4924s)


### 用 ReLU 作为非线性函数

对于有很多层的网络，ReLU 的效果最好，甚至使其他函数如 sigmoid、hyperbolic tangent $\tanh(\cdot)$ 相形之下过时了。ReLU 很有效的原因可能是因为它具有的一个尖点使它具有缩放的等变性。

### 用交叉熵作为分类问题的损失函数

在讲座里前面提到的 Log softmax是交叉熵损失的特例。Pytorch 里，请确认传给交叉熵损失函数时要使用 *Log* softmax 为输入（而非一般 softmax）。


### 训练时使用小批量（minibatch）的随机梯度下降

如同之前所讨论的，小批量使你能更有效率的训练，因为资料中有重复；你不需要每一步对每个观察进行预测、计算损失以估计梯度。


### Shuffle the order of the training examples when using stochastic gradient descent

Order matters. If the model sees only examples from a single class during each training step, then it will learn to predict that class without learning why it ought to be predicting that class. For example, if you were trying to classify digits from the MNIST dataset and the data was unshuffled, the bias parameters in the last layer would simply always predict zero, then adapt to always predict one, then two, etc. Ideally, you should have samples from every class in every minibatch.

However, there's ongoing debate over whether you need to change the order of the samples in every pass (epoch).


### Normalize the inputs to have zero mean and unit variance

Before training, it's useful to normalize each input feature so that it has a mean of zero and a standard deviation of one. When using RGB image data, it is common to take mean and standard deviation of each channel individually and normalize the image channel-wise. For example, take the mean $m_b$ and standard deviation $\sigma_b$ of all the blue values in the dataset, then normalize the blue values for each individual image as.

$$
b_{[i,j]}^{'} = \frac{b_{[i,j]} - m_b}{\max(\sigma_b, \epsilon)}
$$

where $\epsilon$ is an arbitrarily small number that we use to avoid division by zero. Repeat the same for green and red channels. This is necessary to get a meaningful signal out of images taken in different lighting; for example, day lit pictures have a lot of red while underwater pictures have almost none.


### Use a schedule to decrease the learning rate

The learning rate should fall as training goes on. In practice, most advanced models are trained by using algorithms like Adam/Momentum which adapt the learning rate instead of simple SGD with a constant learning rate.


### Use L1 and/or L2 regularization for weight decay

You can add a cost for large weights to the cost function. For example, using L2 regularization, we would define the loss $L$ and update the weights $w$ as follows:

$$
L(S, w) = C(S, w) + \alpha \Vert w \Vert^2\\
\frac{\partial R}{\partial w_i} = 2w_i\\
w_i = w_i - \eta\frac{\partial C}{\partial w_i} = w_i - \eta(\frac{\partial C}{\partial w_i} + 2 \alpha w_i)
$$

To understand why this is called weight decay, note that we can rewrite the above formula to show that we multiply $w_i$ by a constant less than one during the update.

$$
w_i = (1 - 2 \eta \alpha) w_i - \eta\frac{\partial C}{\partial w_i}
$$

L1 regularization (Lasso) is similar, except that we use $\sum_i \vert w_i\vert$ instead of $\Vert w \Vert^2$.

Essentially, regularization tries to tell the system to minimize the cost function with the shortest weight vector possible. With L1 regularization, weights that are not useful are shrunk to 0.


### Weight initialisation

The weights need to be initialised at random, however, they shouldn't be too large or too small such that output is roughly of the same variance as that of input. There are various weight initialisation tricks built into PyTorch. One of the tricks that works well for deep models is Kaiming initialisation where the variance of the weights is inversely proportional to square root of number of inputs.


### Use dropout

Dropout is another form of regularization. It can be thought of as another layer of the neural net: it takes inputs, randomly sets $n/2$ of the inputs to zero, and returns the result as output. This forces the system to take information from all input units rather than becoming overly reliant on a small number of input units thus distributing the information across all of the units in a layer. This method was initially proposed by <a href="https://arxiv.org/abs/1207.0580">Hinton et al (2012)</a>.

For more tricks, see  <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">LeCun et al 1998</a>.

Finally, note that backpropagation doesn't just work for stacked models; it can work for any directed acyclic graph (DAG) as long as there is a partial order on the modules.

