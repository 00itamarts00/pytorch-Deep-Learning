{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation in NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from tensorflow import keras as kr \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set my plotting style\n",
    "plt.style.use(('dark_background', 'bmh'))\n",
    "plt.rc('axes', facecolor='none')\n",
    "plt.rc('figure', figsize=(16, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113bd0df0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcuts\n",
    "imdb = kr.datasets.imdb\n",
    "Tokeniser = kr.preprocessing.text.Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features we want\n",
    "features_nb = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=features_nb)\n",
    "\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokeniser = Tokeniser(num_words=features_nb)\n",
    "train_features = tokeniser.sequences_to_matrix(train_data, mode='binary')\n",
    "test_features = tokeniser.sequences_to_matrix(test_data, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploring the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (25000,)\n",
      "train_target.shape: (25000,)\n",
      "test_data.shape: (25000,)\n",
      "test_target.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Check data set sizes\n",
    "print('train_data.shape:', train_data.shape)\n",
    "print('train_target.shape:', train_target.shape)\n",
    "print('test_data.shape:', test_data.shape)\n",
    "print('test_target.shape:', test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train_data[0]): <class 'list'>\n",
      "type(train_target[0]): <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# Check format of first training sample\n",
    "print('type(train_data[0]):', type(train_data[0]))\n",
    "print('type(train_target[0]):', type(train_target[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews length: [218, 189, 141, 550, 147, 43, 123, 562, 233, 130]\n",
      "Review sentiment (bad/good): [1 0 0 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Check size of first 10 training samples and corresponding target\n",
    "print('Reviews length:', [len(sample) for sample in train_data[:10]])\n",
    "print('Review sentiment (bad/good):', train_target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# Show first review - machine format\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set text visualisation helper function\n",
    "def show_text(sample):\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    word_to_id[\"<PAD>\"] = 0\n",
    "    word_to_id[\"<START>\"] = 1\n",
    "    word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "    print(' '.join(id_to_word[id_] for id_ in sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting <UNK> <UNK> story direction <UNK> really <UNK> the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> <UNK> as myself so i loved the fact there was a real <UNK> with this film the <UNK> <UNK> throughout the film were great it was just brilliant so much that i <UNK> the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the <UNK> <UNK> was amazing really <UNK> at the end it was so sad and you know what they say if you <UNK> at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of <UNK> and paul they were just brilliant children are often left out of the <UNK> <UNK> i think because the stars that play them all <UNK> up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so <UNK> because it was true and was <UNK> life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "# Show first review - human format\n",
    "show_text(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Show first review - neural net format\n",
    "print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   2.   0.   4.   5.   6.   7.   8.   9.   0.   0.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.   0.  21.  22.   0.   0.  25.  26.   0.\n",
      "  28.   0.  30.   0.  32.  33.   0.  35.  36.   0.  38.  39.   0.   0.\n",
      "   0.  43.   0.   0.  46.   0.  48.   0.  50.  51.  52.   0.   0.   0.\n",
      "  56.   0.   0.   0.   0.   0.  62.   0.   0.  65.  66.   0.   0.   0.\n",
      "   0.  71.   0.   0.   0.   0.  76.  77.   0.   0.   0.   0.  82.   0.\n",
      "   0.   0.   0.  87.  88.   0.   0.   0.  92.   0.   0.   0.   0.   0.\n",
      "  98.   0. 100.   0.   0. 103. 104.   0. 106. 107.   0.   0.   0.   0.\n",
      " 112. 113.   0.   0.   0. 117.   0.   0.   0.   0.   0.   0. 124.   0.\n",
      "   0.   0.   0.   0. 130.   0.   0.   0. 134. 135.   0.   0.   0.   0.\n",
      "   0. 141.   0.   0. 144.   0.   0. 147.   0.   0. 150.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 167.\n",
      "   0.   0.   0.   0. 172. 173.   0.   0.   0.   0. 178.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 192.   0. 194.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0. 215.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 224.   0. 226.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 256.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 283. 284.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 297.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 316. 317.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 336.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 381.   0.   0.   0. 385. 386.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 400.   0.   0.   0.   0.   0.\n",
      "   0. 407.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 447.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 458.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 469.   0.   0.   0.   0.   0.   0.\n",
      " 476.   0.   0.   0. 480.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 515.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 530.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 546.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 619.   0.   0.   0.   0.   0.   0. 626.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 670.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0. 723.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 838.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 973.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "# Show first review - neural net format - explanation\n",
    "print(train_features[0] * np.arange(len(train_features[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring regularisation of NN\n",
    "\n",
    "Play with the code, especially the one marked `# toggle`.  \n",
    "Start from `# toggle 0`, and then, one at the time, `# toggle 1` to `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerDense(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, units_size):\n",
    "\n",
    "        super(ThreeLayerDense, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, units_size) #features_nb, 16\n",
    "        self.linear2 = torch.nn.Linear(units_size, units_size)\n",
    "        #self.dropout\n",
    "        self.linear3 = torch.nn.Linear(units_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.linear2(x) \n",
    "        x = F.relu(x)\n",
    "        #Add dropout regularization\n",
    "        #x = F.dropout(x, training=self.training)   \n",
    "        return nn.Sigmoid()(self.linear3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "log_interval = 10\n",
    "batch_size = 100\n",
    "\n",
    "model = ThreeLayerDense(features_nb, 16)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "log_interval = 10\n",
    "batch_size = 100\n",
    "\n",
    "model = ThreeLayerDense(features_nb, 16)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "#l2 regularization\n",
    "#l2_regularization_factor = 0.0005\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay = l2_regularization_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, mini-batch 0 of 25000, training loss: 0.613348\n",
      "Train Epoch: 0, mini-batch 10 of 25000, training loss: 0.656209\n",
      "Train Epoch: 0, mini-batch 20 of 25000, training loss: 0.729808\n",
      "Train Epoch: 0, mini-batch 30 of 25000, training loss: 0.687957\n",
      "Train Epoch: 0, mini-batch 40 of 25000, training loss: 0.726682\n",
      "Train Epoch: 0, mini-batch 50 of 25000, training loss: 0.679756\n",
      "Train Epoch: 0, mini-batch 60 of 25000, training loss: 0.630801\n",
      "Train Epoch: 0, mini-batch 70 of 25000, training loss: 0.644291\n",
      "Train Epoch: 0, mini-batch 80 of 25000, training loss: 0.671881\n",
      "Train Epoch: 0, mini-batch 90 of 25000, training loss: 0.689763\n",
      "Train Epoch: 0, mini-batch 100 of 25000, training loss: 0.543776\n",
      "Train Epoch: 0, mini-batch 110 of 25000, training loss: 0.526624\n",
      "Train Epoch: 0, mini-batch 120 of 25000, training loss: 0.737881\n",
      "Train Epoch: 0, mini-batch 130 of 25000, training loss: 0.428849\n",
      "Train Epoch: 0, mini-batch 140 of 25000, training loss: 0.891402\n",
      "Train Epoch: 0, mini-batch 150 of 25000, training loss: 0.807821\n",
      "Train Epoch: 0, mini-batch 160 of 25000, training loss: 0.539858\n",
      "Train Epoch: 0, mini-batch 170 of 25000, training loss: 0.731582\n",
      "Train Epoch: 0, mini-batch 180 of 25000, training loss: 0.752370\n",
      "Train Epoch: 0, mini-batch 190 of 25000, training loss: 0.548617\n",
      "Train Epoch: 0, mini-batch 200 of 25000, training loss: 0.912223\n",
      "Train Epoch: 0, mini-batch 210 of 25000, training loss: 0.780212\n",
      "Train Epoch: 0, mini-batch 220 of 25000, training loss: 0.702676\n",
      "Train Epoch: 0, mini-batch 230 of 25000, training loss: 0.660076\n",
      "Train Epoch: 0, mini-batch 240 of 25000, training loss: 0.633418\n",
      "Train Epoch: 0, mini-batch 250 of 25000, training loss: 0.867424\n",
      "Train Epoch: 0, mini-batch 260 of 25000, training loss: 0.837694\n",
      "Train Epoch: 0, mini-batch 270 of 25000, training loss: 0.527325\n",
      "Train Epoch: 0, mini-batch 280 of 25000, training loss: 0.449315\n",
      "Train Epoch: 0, mini-batch 290 of 25000, training loss: 0.743231\n",
      "Train Epoch: 0, mini-batch 300 of 25000, training loss: 0.765246\n",
      "Train Epoch: 0, mini-batch 310 of 25000, training loss: 0.598238\n",
      "Train Epoch: 0, mini-batch 320 of 25000, training loss: 0.631316\n",
      "Train Epoch: 0, mini-batch 330 of 25000, training loss: 0.603322\n",
      "Train Epoch: 0, mini-batch 340 of 25000, training loss: 0.894791\n",
      "Train Epoch: 0, mini-batch 350 of 25000, training loss: 0.862421\n",
      "Train Epoch: 0, mini-batch 360 of 25000, training loss: 0.628720\n",
      "Train Epoch: 0, mini-batch 370 of 25000, training loss: 0.668555\n",
      "Train Epoch: 0, mini-batch 380 of 25000, training loss: 0.599831\n",
      "Train Epoch: 0, mini-batch 390 of 25000, training loss: 0.859084\n",
      "Train Epoch: 0, mini-batch 400 of 25000, training loss: 0.563479\n",
      "Train Epoch: 0, mini-batch 410 of 25000, training loss: 0.546648\n",
      "Train Epoch: 0, mini-batch 420 of 25000, training loss: 0.562242\n",
      "Train Epoch: 0, mini-batch 430 of 25000, training loss: 0.759669\n",
      "Train Epoch: 0, mini-batch 440 of 25000, training loss: 0.438840\n",
      "Train Epoch: 0, mini-batch 450 of 25000, training loss: 0.501760\n",
      "Train Epoch: 0, mini-batch 460 of 25000, training loss: 0.586348\n",
      "Train Epoch: 0, mini-batch 470 of 25000, training loss: 0.438942\n",
      "Train Epoch: 0, mini-batch 480 of 25000, training loss: 0.326664\n",
      "Train Epoch: 0, mini-batch 490 of 25000, training loss: 0.514596\n",
      "Train Epoch: 0, mini-batch 500 of 25000, training loss: 0.607255\n",
      "Train Epoch: 0, mini-batch 510 of 25000, training loss: 0.318529\n",
      "Train Epoch: 0, mini-batch 520 of 25000, training loss: 0.491454\n",
      "Train Epoch: 0, mini-batch 530 of 25000, training loss: 1.079304\n",
      "Train Epoch: 0, mini-batch 540 of 25000, training loss: 0.839262\n",
      "Train Epoch: 0, mini-batch 550 of 25000, training loss: 0.371589\n",
      "Train Epoch: 0, mini-batch 560 of 25000, training loss: 0.625780\n",
      "Train Epoch: 0, mini-batch 570 of 25000, training loss: 0.443470\n",
      "Train Epoch: 0, mini-batch 580 of 25000, training loss: 0.125958\n",
      "Train Epoch: 0, mini-batch 590 of 25000, training loss: 0.242208\n",
      "Train Epoch: 0, mini-batch 600 of 25000, training loss: 0.282231\n",
      "Train Epoch: 0, mini-batch 610 of 25000, training loss: 0.224740\n",
      "Train Epoch: 0, mini-batch 620 of 25000, training loss: 0.210131\n",
      "Train Epoch: 0, mini-batch 630 of 25000, training loss: 0.616395\n",
      "Train Epoch: 0, mini-batch 640 of 25000, training loss: 0.390733\n",
      "Train Epoch: 0, mini-batch 650 of 25000, training loss: 0.561544\n",
      "Train Epoch: 0, mini-batch 660 of 25000, training loss: 0.502752\n",
      "Train Epoch: 0, mini-batch 670 of 25000, training loss: 1.495242\n",
      "Train Epoch: 0, mini-batch 680 of 25000, training loss: 0.400808\n",
      "Train Epoch: 0, mini-batch 690 of 25000, training loss: 0.694864\n",
      "Train Epoch: 0, mini-batch 700 of 25000, training loss: 0.679132\n",
      "Train Epoch: 0, mini-batch 710 of 25000, training loss: 0.113149\n",
      "Train Epoch: 0, mini-batch 720 of 25000, training loss: 0.177891\n",
      "Train Epoch: 0, mini-batch 730 of 25000, training loss: 1.090894\n",
      "Train Epoch: 0, mini-batch 740 of 25000, training loss: 0.166030\n",
      "Train Epoch: 0, mini-batch 750 of 25000, training loss: 0.394797\n",
      "Train Epoch: 0, mini-batch 760 of 25000, training loss: 0.309393\n",
      "Train Epoch: 0, mini-batch 770 of 25000, training loss: 0.076825\n",
      "Train Epoch: 0, mini-batch 780 of 25000, training loss: 0.932243\n",
      "Train Epoch: 0, mini-batch 790 of 25000, training loss: 0.520934\n",
      "Train Epoch: 0, mini-batch 800 of 25000, training loss: 0.344084\n",
      "Train Epoch: 0, mini-batch 810 of 25000, training loss: 0.927929\n",
      "Train Epoch: 0, mini-batch 820 of 25000, training loss: 0.177988\n",
      "Train Epoch: 0, mini-batch 830 of 25000, training loss: 0.525280\n",
      "Train Epoch: 0, mini-batch 840 of 25000, training loss: 1.423837\n",
      "Train Epoch: 0, mini-batch 850 of 25000, training loss: 0.326039\n",
      "Train Epoch: 0, mini-batch 860 of 25000, training loss: 0.155607\n",
      "Train Epoch: 0, mini-batch 870 of 25000, training loss: 0.059481\n",
      "Train Epoch: 0, mini-batch 880 of 25000, training loss: 0.699451\n",
      "Train Epoch: 0, mini-batch 890 of 25000, training loss: 0.385052\n",
      "Train Epoch: 0, mini-batch 900 of 25000, training loss: 0.297237\n",
      "Train Epoch: 0, mini-batch 910 of 25000, training loss: 0.088602\n",
      "Train Epoch: 0, mini-batch 920 of 25000, training loss: 0.170798\n",
      "Train Epoch: 0, mini-batch 930 of 25000, training loss: 0.323096\n",
      "Train Epoch: 0, mini-batch 940 of 25000, training loss: 0.075472\n",
      "Train Epoch: 0, mini-batch 950 of 25000, training loss: 0.268579\n",
      "Train Epoch: 0, mini-batch 960 of 25000, training loss: 1.168639\n",
      "Train Epoch: 0, mini-batch 970 of 25000, training loss: 0.290125\n",
      "Train Epoch: 0, mini-batch 980 of 25000, training loss: 0.426847\n",
      "Train Epoch: 0, mini-batch 990 of 25000, training loss: 0.371328\n",
      "Train Epoch: 0, mini-batch 1000 of 25000, training loss: 0.508119\n",
      "Train Epoch: 0, mini-batch 1010 of 25000, training loss: 1.165103\n",
      "Train Epoch: 0, mini-batch 1020 of 25000, training loss: 0.069092\n",
      "Train Epoch: 0, mini-batch 1030 of 25000, training loss: 0.143665\n",
      "Train Epoch: 0, mini-batch 1040 of 25000, training loss: 1.081785\n",
      "Train Epoch: 0, mini-batch 1050 of 25000, training loss: 1.250427\n",
      "Train Epoch: 0, mini-batch 1060 of 25000, training loss: 0.138917\n",
      "Train Epoch: 0, mini-batch 1070 of 25000, training loss: 0.938945\n",
      "Train Epoch: 0, mini-batch 1080 of 25000, training loss: 0.367828\n",
      "Train Epoch: 0, mini-batch 1090 of 25000, training loss: 0.073408\n",
      "Train Epoch: 0, mini-batch 1100 of 25000, training loss: 0.176662\n",
      "Train Epoch: 0, mini-batch 1110 of 25000, training loss: 0.023571\n",
      "Train Epoch: 0, mini-batch 1120 of 25000, training loss: 0.201785\n",
      "Train Epoch: 0, mini-batch 1130 of 25000, training loss: 0.148846\n",
      "Train Epoch: 0, mini-batch 1140 of 25000, training loss: 0.150723\n",
      "Train Epoch: 0, mini-batch 1150 of 25000, training loss: 0.102518\n",
      "Train Epoch: 0, mini-batch 1160 of 25000, training loss: 1.139248\n",
      "Train Epoch: 0, mini-batch 1170 of 25000, training loss: 0.581006\n",
      "Train Epoch: 0, mini-batch 1180 of 25000, training loss: 0.174660\n",
      "Train Epoch: 0, mini-batch 1190 of 25000, training loss: 0.089821\n",
      "Train Epoch: 0, mini-batch 1200 of 25000, training loss: 0.083362\n",
      "Train Epoch: 0, mini-batch 1210 of 25000, training loss: 0.866724\n",
      "Train Epoch: 0, mini-batch 1220 of 25000, training loss: 1.065022\n",
      "Train Epoch: 0, mini-batch 1230 of 25000, training loss: 0.085668\n",
      "Train Epoch: 0, mini-batch 1240 of 25000, training loss: 0.105777\n",
      "Train Epoch: 0, mini-batch 1250 of 25000, training loss: 0.238263\n",
      "Train Epoch: 0, mini-batch 1260 of 25000, training loss: 0.100773\n",
      "Train Epoch: 0, mini-batch 1270 of 25000, training loss: 0.017797\n",
      "Train Epoch: 0, mini-batch 1280 of 25000, training loss: 0.438620\n",
      "Train Epoch: 0, mini-batch 1290 of 25000, training loss: 0.090140\n",
      "Train Epoch: 0, mini-batch 1300 of 25000, training loss: 0.351111\n",
      "Train Epoch: 0, mini-batch 1310 of 25000, training loss: 0.367963\n",
      "Train Epoch: 0, mini-batch 1320 of 25000, training loss: 0.141744\n",
      "Train Epoch: 0, mini-batch 1330 of 25000, training loss: 0.597231\n",
      "Train Epoch: 0, mini-batch 1340 of 25000, training loss: 0.430838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, mini-batch 1350 of 25000, training loss: 0.213357\n",
      "Train Epoch: 0, mini-batch 1360 of 25000, training loss: 0.549979\n",
      "Train Epoch: 0, mini-batch 1370 of 25000, training loss: 0.807515\n",
      "Train Epoch: 0, mini-batch 1380 of 25000, training loss: 0.105247\n",
      "Train Epoch: 0, mini-batch 1390 of 25000, training loss: 0.098139\n",
      "Train Epoch: 0, mini-batch 1400 of 25000, training loss: 0.690399\n",
      "Train Epoch: 0, mini-batch 1410 of 25000, training loss: 0.327312\n",
      "Train Epoch: 0, mini-batch 1420 of 25000, training loss: 0.348850\n",
      "Train Epoch: 0, mini-batch 1430 of 25000, training loss: 0.106565\n",
      "Train Epoch: 0, mini-batch 1440 of 25000, training loss: 0.616981\n",
      "Train Epoch: 0, mini-batch 1450 of 25000, training loss: 0.138042\n",
      "Train Epoch: 0, mini-batch 1460 of 25000, training loss: 0.238999\n",
      "Train Epoch: 0, mini-batch 1470 of 25000, training loss: 0.087767\n",
      "Train Epoch: 0, mini-batch 1480 of 25000, training loss: 0.026745\n",
      "Train Epoch: 0, mini-batch 1490 of 25000, training loss: 0.141960\n",
      "Train Epoch: 0, mini-batch 1500 of 25000, training loss: 0.049467\n",
      "Train Epoch: 0, mini-batch 1510 of 25000, training loss: 0.067837\n",
      "Train Epoch: 0, mini-batch 1520 of 25000, training loss: 0.164953\n",
      "Train Epoch: 0, mini-batch 1530 of 25000, training loss: 0.206287\n",
      "Train Epoch: 0, mini-batch 1540 of 25000, training loss: 0.363144\n",
      "Train Epoch: 0, mini-batch 1550 of 25000, training loss: 0.551098\n",
      "Train Epoch: 0, mini-batch 1560 of 25000, training loss: 0.582295\n",
      "Train Epoch: 0, mini-batch 1570 of 25000, training loss: 1.130321\n",
      "Train Epoch: 0, mini-batch 1580 of 25000, training loss: 0.634868\n",
      "Train Epoch: 0, mini-batch 1590 of 25000, training loss: 0.310052\n",
      "Train Epoch: 0, mini-batch 1600 of 25000, training loss: 0.051600\n",
      "Train Epoch: 0, mini-batch 1610 of 25000, training loss: 0.074433\n",
      "Train Epoch: 0, mini-batch 1620 of 25000, training loss: 0.448036\n",
      "Train Epoch: 0, mini-batch 1630 of 25000, training loss: 0.014634\n",
      "Train Epoch: 0, mini-batch 1640 of 25000, training loss: 0.102333\n",
      "Train Epoch: 0, mini-batch 1650 of 25000, training loss: 0.649336\n",
      "Train Epoch: 0, mini-batch 1660 of 25000, training loss: 0.205136\n",
      "Train Epoch: 0, mini-batch 1670 of 25000, training loss: 0.114491\n",
      "Train Epoch: 0, mini-batch 1680 of 25000, training loss: 0.108183\n",
      "Train Epoch: 0, mini-batch 1690 of 25000, training loss: 0.076561\n",
      "Train Epoch: 0, mini-batch 1700 of 25000, training loss: 0.118437\n",
      "Train Epoch: 0, mini-batch 1710 of 25000, training loss: 0.463084\n",
      "Train Epoch: 0, mini-batch 1720 of 25000, training loss: 0.133806\n",
      "Train Epoch: 0, mini-batch 1730 of 25000, training loss: 0.400703\n",
      "Train Epoch: 0, mini-batch 1740 of 25000, training loss: 0.179780\n",
      "Train Epoch: 0, mini-batch 1750 of 25000, training loss: 0.078027\n",
      "Train Epoch: 0, mini-batch 1760 of 25000, training loss: 0.067657\n",
      "Train Epoch: 0, mini-batch 1770 of 25000, training loss: 0.438776\n",
      "Train Epoch: 0, mini-batch 1780 of 25000, training loss: 0.311761\n",
      "Train Epoch: 0, mini-batch 1790 of 25000, training loss: 0.038656\n",
      "Train Epoch: 0, mini-batch 1800 of 25000, training loss: 0.053391\n",
      "Train Epoch: 0, mini-batch 1810 of 25000, training loss: 0.175194\n",
      "Train Epoch: 0, mini-batch 1820 of 25000, training loss: 1.033062\n",
      "Train Epoch: 0, mini-batch 1830 of 25000, training loss: 0.044829\n",
      "Train Epoch: 0, mini-batch 1840 of 25000, training loss: 0.683936\n",
      "Train Epoch: 0, mini-batch 1850 of 25000, training loss: 1.128453\n",
      "Train Epoch: 0, mini-batch 1860 of 25000, training loss: 0.058808\n",
      "Train Epoch: 0, mini-batch 1870 of 25000, training loss: 0.734471\n",
      "Train Epoch: 0, mini-batch 1880 of 25000, training loss: 2.200532\n",
      "Train Epoch: 0, mini-batch 1890 of 25000, training loss: 0.131305\n",
      "Train Epoch: 0, mini-batch 1900 of 25000, training loss: 0.528457\n",
      "Train Epoch: 0, mini-batch 1910 of 25000, training loss: 0.237458\n",
      "Train Epoch: 0, mini-batch 1920 of 25000, training loss: 0.070865\n",
      "Train Epoch: 0, mini-batch 1930 of 25000, training loss: 0.038353\n",
      "Train Epoch: 0, mini-batch 1940 of 25000, training loss: 0.057711\n",
      "Train Epoch: 0, mini-batch 1950 of 25000, training loss: 0.409675\n",
      "Train Epoch: 0, mini-batch 1960 of 25000, training loss: 0.086480\n",
      "Train Epoch: 0, mini-batch 1970 of 25000, training loss: 0.062472\n",
      "Train Epoch: 0, mini-batch 1980 of 25000, training loss: 1.290372\n",
      "Train Epoch: 0, mini-batch 1990 of 25000, training loss: 0.111890\n",
      "Train Epoch: 0, mini-batch 2000 of 25000, training loss: 0.116587\n",
      "Train Epoch: 0, mini-batch 2010 of 25000, training loss: 0.070448\n",
      "Train Epoch: 0, mini-batch 2020 of 25000, training loss: 1.295603\n",
      "Train Epoch: 0, mini-batch 2030 of 25000, training loss: 0.013155\n",
      "Train Epoch: 0, mini-batch 2040 of 25000, training loss: 0.039406\n",
      "Train Epoch: 0, mini-batch 2050 of 25000, training loss: 0.327565\n",
      "Train Epoch: 0, mini-batch 2060 of 25000, training loss: 0.190103\n",
      "Train Epoch: 0, mini-batch 2070 of 25000, training loss: 0.002528\n",
      "Train Epoch: 0, mini-batch 2080 of 25000, training loss: 0.306330\n",
      "Train Epoch: 0, mini-batch 2090 of 25000, training loss: 0.074061\n",
      "Train Epoch: 0, mini-batch 2100 of 25000, training loss: 0.116784\n",
      "Train Epoch: 0, mini-batch 2110 of 25000, training loss: 0.065462\n",
      "Train Epoch: 0, mini-batch 2120 of 25000, training loss: 0.077672\n",
      "Train Epoch: 0, mini-batch 2130 of 25000, training loss: 0.615535\n",
      "Train Epoch: 0, mini-batch 2140 of 25000, training loss: 0.110104\n",
      "Train Epoch: 0, mini-batch 2150 of 25000, training loss: 0.007132\n",
      "Train Epoch: 0, mini-batch 2160 of 25000, training loss: 0.092492\n",
      "Train Epoch: 0, mini-batch 2170 of 25000, training loss: 0.188916\n",
      "Train Epoch: 0, mini-batch 2180 of 25000, training loss: 0.395418\n",
      "Train Epoch: 0, mini-batch 2190 of 25000, training loss: 2.316415\n",
      "Train Epoch: 0, mini-batch 2200 of 25000, training loss: 0.967004\n",
      "Train Epoch: 0, mini-batch 2210 of 25000, training loss: 0.087988\n",
      "Train Epoch: 0, mini-batch 2220 of 25000, training loss: 0.230925\n",
      "Train Epoch: 0, mini-batch 2230 of 25000, training loss: 0.349449\n",
      "Train Epoch: 0, mini-batch 2240 of 25000, training loss: 0.581396\n",
      "Train Epoch: 0, mini-batch 2250 of 25000, training loss: 0.007030\n",
      "Train Epoch: 0, mini-batch 2260 of 25000, training loss: 0.267469\n",
      "Train Epoch: 0, mini-batch 2270 of 25000, training loss: 1.257884\n",
      "Train Epoch: 0, mini-batch 2280 of 25000, training loss: 0.130471\n",
      "Train Epoch: 0, mini-batch 2290 of 25000, training loss: 2.163872\n",
      "Train Epoch: 0, mini-batch 2300 of 25000, training loss: 0.649355\n",
      "Train Epoch: 0, mini-batch 2310 of 25000, training loss: 0.594898\n",
      "Train Epoch: 0, mini-batch 2320 of 25000, training loss: 0.202869\n",
      "Train Epoch: 0, mini-batch 2330 of 25000, training loss: 0.309053\n",
      "Train Epoch: 0, mini-batch 2340 of 25000, training loss: 0.030208\n",
      "Train Epoch: 0, mini-batch 2350 of 25000, training loss: 1.286901\n",
      "Train Epoch: 0, mini-batch 2360 of 25000, training loss: 0.720747\n",
      "Train Epoch: 0, mini-batch 2370 of 25000, training loss: 1.291484\n",
      "Train Epoch: 0, mini-batch 2380 of 25000, training loss: 0.150133\n",
      "Train Epoch: 0, mini-batch 2390 of 25000, training loss: 0.448220\n",
      "Train Epoch: 0, mini-batch 2400 of 25000, training loss: 2.511184\n",
      "Train Epoch: 0, mini-batch 2410 of 25000, training loss: 1.175108\n",
      "Train Epoch: 0, mini-batch 2420 of 25000, training loss: 0.030802\n",
      "Train Epoch: 0, mini-batch 2430 of 25000, training loss: 0.975961\n",
      "Train Epoch: 0, mini-batch 2440 of 25000, training loss: 0.043852\n",
      "Train Epoch: 0, mini-batch 2450 of 25000, training loss: 0.263311\n",
      "Train Epoch: 0, mini-batch 2460 of 25000, training loss: 0.729234\n",
      "Train Epoch: 0, mini-batch 2470 of 25000, training loss: 1.260579\n",
      "Train Epoch: 0, mini-batch 2480 of 25000, training loss: 0.055387\n",
      "Train Epoch: 0, mini-batch 2490 of 25000, training loss: 0.341767\n",
      "Train Epoch: 0, mini-batch 2500 of 25000, training loss: 1.280565\n",
      "Train Epoch: 0, mini-batch 2510 of 25000, training loss: 1.940875\n",
      "Train Epoch: 0, mini-batch 2520 of 25000, training loss: 0.090997\n",
      "Train Epoch: 0, mini-batch 2530 of 25000, training loss: 0.424867\n",
      "Train Epoch: 0, mini-batch 2540 of 25000, training loss: 4.075252\n",
      "Train Epoch: 0, mini-batch 2550 of 25000, training loss: 0.186282\n",
      "Train Epoch: 0, mini-batch 2560 of 25000, training loss: 0.048789\n",
      "Train Epoch: 0, mini-batch 2570 of 25000, training loss: 0.561172\n",
      "Train Epoch: 0, mini-batch 2580 of 25000, training loss: 0.267773\n",
      "Train Epoch: 0, mini-batch 2590 of 25000, training loss: 1.190111\n",
      "Train Epoch: 0, mini-batch 2600 of 25000, training loss: 0.940265\n",
      "Train Epoch: 0, mini-batch 2610 of 25000, training loss: 0.879053\n",
      "Train Epoch: 0, mini-batch 2620 of 25000, training loss: 0.008038\n",
      "Train Epoch: 0, mini-batch 2630 of 25000, training loss: 0.246046\n",
      "Train Epoch: 0, mini-batch 2640 of 25000, training loss: 0.005735\n",
      "Train Epoch: 0, mini-batch 2650 of 25000, training loss: 1.260081\n",
      "Train Epoch: 0, mini-batch 2660 of 25000, training loss: 0.074785\n",
      "Train Epoch: 0, mini-batch 2670 of 25000, training loss: 0.217115\n",
      "Train Epoch: 0, mini-batch 2680 of 25000, training loss: 0.078779\n",
      "Train Epoch: 0, mini-batch 2690 of 25000, training loss: 1.674913\n",
      "Train Epoch: 0, mini-batch 2700 of 25000, training loss: 0.153458\n",
      "Train Epoch: 0, mini-batch 2710 of 25000, training loss: 0.433151\n",
      "Train Epoch: 0, mini-batch 2720 of 25000, training loss: 0.039543\n",
      "Train Epoch: 0, mini-batch 2730 of 25000, training loss: 0.406781\n",
      "Train Epoch: 0, mini-batch 2740 of 25000, training loss: 0.463568\n",
      "Train Epoch: 0, mini-batch 2750 of 25000, training loss: 0.106969\n",
      "Train Epoch: 0, mini-batch 2760 of 25000, training loss: 0.343007\n",
      "Train Epoch: 0, mini-batch 2770 of 25000, training loss: 0.401166\n",
      "Train Epoch: 0, mini-batch 2780 of 25000, training loss: 0.004519\n",
      "Train Epoch: 0, mini-batch 2790 of 25000, training loss: 0.292079\n",
      "Train Epoch: 0, mini-batch 2800 of 25000, training loss: 0.154109\n",
      "Train Epoch: 0, mini-batch 2810 of 25000, training loss: 0.402659\n",
      "Train Epoch: 0, mini-batch 2820 of 25000, training loss: 1.153210\n",
      "Train Epoch: 0, mini-batch 2830 of 25000, training loss: 0.119693\n",
      "Train Epoch: 0, mini-batch 2840 of 25000, training loss: 0.037723\n",
      "Train Epoch: 0, mini-batch 2850 of 25000, training loss: 0.195785\n",
      "Train Epoch: 0, mini-batch 2860 of 25000, training loss: 0.167904\n",
      "Train Epoch: 0, mini-batch 2870 of 25000, training loss: 0.128426\n",
      "Train Epoch: 0, mini-batch 2880 of 25000, training loss: 0.248851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, mini-batch 2890 of 25000, training loss: 0.030071\n",
      "Train Epoch: 0, mini-batch 2900 of 25000, training loss: 0.415149\n",
      "Train Epoch: 0, mini-batch 2910 of 25000, training loss: 0.049818\n",
      "Train Epoch: 0, mini-batch 2920 of 25000, training loss: 0.036722\n",
      "Train Epoch: 0, mini-batch 2930 of 25000, training loss: 1.542135\n",
      "Train Epoch: 0, mini-batch 2940 of 25000, training loss: 0.329811\n",
      "Train Epoch: 0, mini-batch 2950 of 25000, training loss: 0.027226\n",
      "Train Epoch: 0, mini-batch 2960 of 25000, training loss: 0.102297\n",
      "Train Epoch: 0, mini-batch 2970 of 25000, training loss: 0.032228\n",
      "Train Epoch: 0, mini-batch 2980 of 25000, training loss: 1.619554\n",
      "Train Epoch: 0, mini-batch 2990 of 25000, training loss: 0.867965\n",
      "Train Epoch: 0, mini-batch 3000 of 25000, training loss: 0.913068\n",
      "Train Epoch: 0, mini-batch 3010 of 25000, training loss: 0.186353\n",
      "Train Epoch: 0, mini-batch 3020 of 25000, training loss: 0.084611\n",
      "Train Epoch: 0, mini-batch 3030 of 25000, training loss: 0.016289\n",
      "Train Epoch: 0, mini-batch 3040 of 25000, training loss: 0.195504\n",
      "Train Epoch: 0, mini-batch 3050 of 25000, training loss: 0.313801\n",
      "Train Epoch: 0, mini-batch 3060 of 25000, training loss: 0.025509\n",
      "Train Epoch: 0, mini-batch 3070 of 25000, training loss: 0.526915\n",
      "Train Epoch: 0, mini-batch 3080 of 25000, training loss: 0.022147\n",
      "Train Epoch: 0, mini-batch 3090 of 25000, training loss: 0.032785\n",
      "Train Epoch: 0, mini-batch 3100 of 25000, training loss: 1.273104\n",
      "Train Epoch: 0, mini-batch 3110 of 25000, training loss: 0.232449\n",
      "Train Epoch: 0, mini-batch 3120 of 25000, training loss: 0.045929\n",
      "Train Epoch: 0, mini-batch 3130 of 25000, training loss: 0.351597\n",
      "Train Epoch: 0, mini-batch 3140 of 25000, training loss: 0.064361\n",
      "Train Epoch: 0, mini-batch 3150 of 25000, training loss: 0.026777\n",
      "Train Epoch: 0, mini-batch 3160 of 25000, training loss: 1.198997\n",
      "Train Epoch: 0, mini-batch 3170 of 25000, training loss: 0.527220\n",
      "Train Epoch: 0, mini-batch 3180 of 25000, training loss: 0.099627\n",
      "Train Epoch: 0, mini-batch 3190 of 25000, training loss: 0.527760\n",
      "Train Epoch: 0, mini-batch 3200 of 25000, training loss: 0.210407\n",
      "Train Epoch: 0, mini-batch 3210 of 25000, training loss: 0.084859\n",
      "Train Epoch: 0, mini-batch 3220 of 25000, training loss: 0.134921\n",
      "Train Epoch: 0, mini-batch 3230 of 25000, training loss: 1.345753\n",
      "Train Epoch: 0, mini-batch 3240 of 25000, training loss: 0.020164\n",
      "Train Epoch: 0, mini-batch 3250 of 25000, training loss: 0.087374\n",
      "Train Epoch: 0, mini-batch 3260 of 25000, training loss: 0.596103\n",
      "Train Epoch: 0, mini-batch 3270 of 25000, training loss: 0.016561\n",
      "Train Epoch: 0, mini-batch 3280 of 25000, training loss: 0.208378\n",
      "Train Epoch: 0, mini-batch 3290 of 25000, training loss: 0.082171\n",
      "Train Epoch: 0, mini-batch 3300 of 25000, training loss: 0.916363\n",
      "Train Epoch: 0, mini-batch 3310 of 25000, training loss: 0.110043\n",
      "Train Epoch: 0, mini-batch 3320 of 25000, training loss: 0.027511\n",
      "Train Epoch: 0, mini-batch 3330 of 25000, training loss: 0.016699\n",
      "Train Epoch: 0, mini-batch 3340 of 25000, training loss: 0.063624\n",
      "Train Epoch: 0, mini-batch 3350 of 25000, training loss: 0.021714\n",
      "Train Epoch: 0, mini-batch 3360 of 25000, training loss: 0.003081\n",
      "Train Epoch: 0, mini-batch 3370 of 25000, training loss: 0.074608\n",
      "Train Epoch: 0, mini-batch 3380 of 25000, training loss: 0.214823\n",
      "Train Epoch: 0, mini-batch 3390 of 25000, training loss: 0.175921\n",
      "Train Epoch: 0, mini-batch 3400 of 25000, training loss: 0.186537\n",
      "Train Epoch: 0, mini-batch 3410 of 25000, training loss: 0.187861\n",
      "Train Epoch: 0, mini-batch 3420 of 25000, training loss: 0.163338\n",
      "Train Epoch: 0, mini-batch 3430 of 25000, training loss: 0.039451\n",
      "Train Epoch: 0, mini-batch 3440 of 25000, training loss: 0.042109\n",
      "Train Epoch: 0, mini-batch 3450 of 25000, training loss: 0.035083\n",
      "Train Epoch: 0, mini-batch 3460 of 25000, training loss: 3.047846\n",
      "Train Epoch: 0, mini-batch 3470 of 25000, training loss: 0.033768\n",
      "Train Epoch: 0, mini-batch 3480 of 25000, training loss: 0.035671\n",
      "Train Epoch: 0, mini-batch 3490 of 25000, training loss: 0.087327\n",
      "Train Epoch: 0, mini-batch 3500 of 25000, training loss: 0.738941\n",
      "Train Epoch: 0, mini-batch 3510 of 25000, training loss: 0.515239\n",
      "Train Epoch: 0, mini-batch 3520 of 25000, training loss: 1.673963\n",
      "Train Epoch: 0, mini-batch 3530 of 25000, training loss: 0.705778\n",
      "Train Epoch: 0, mini-batch 3540 of 25000, training loss: 0.073616\n",
      "Train Epoch: 0, mini-batch 3550 of 25000, training loss: 0.007983\n",
      "Train Epoch: 0, mini-batch 3560 of 25000, training loss: 0.014097\n",
      "Train Epoch: 0, mini-batch 3570 of 25000, training loss: 1.419500\n",
      "Train Epoch: 0, mini-batch 3580 of 25000, training loss: 0.445345\n",
      "Train Epoch: 0, mini-batch 3590 of 25000, training loss: 0.300114\n",
      "Train Epoch: 0, mini-batch 3600 of 25000, training loss: 0.082057\n",
      "Train Epoch: 0, mini-batch 3610 of 25000, training loss: 0.669807\n",
      "Train Epoch: 0, mini-batch 3620 of 25000, training loss: 0.085891\n",
      "Train Epoch: 0, mini-batch 3630 of 25000, training loss: 0.082098\n",
      "Train Epoch: 0, mini-batch 3640 of 25000, training loss: 1.221681\n",
      "Train Epoch: 0, mini-batch 3650 of 25000, training loss: 4.273271\n",
      "Train Epoch: 0, mini-batch 3660 of 25000, training loss: 0.068786\n",
      "Train Epoch: 0, mini-batch 3670 of 25000, training loss: 0.018628\n",
      "Train Epoch: 0, mini-batch 3680 of 25000, training loss: 0.148327\n",
      "Train Epoch: 0, mini-batch 3690 of 25000, training loss: 0.891867\n",
      "Train Epoch: 0, mini-batch 3700 of 25000, training loss: 4.152328\n",
      "Train Epoch: 0, mini-batch 3710 of 25000, training loss: 0.284857\n",
      "Train Epoch: 0, mini-batch 3720 of 25000, training loss: 0.059660\n",
      "Train Epoch: 0, mini-batch 3730 of 25000, training loss: 0.077040\n",
      "Train Epoch: 0, mini-batch 3740 of 25000, training loss: 0.974248\n",
      "Train Epoch: 0, mini-batch 3750 of 25000, training loss: 2.075559\n",
      "Train Epoch: 0, mini-batch 3760 of 25000, training loss: 0.014420\n",
      "Train Epoch: 0, mini-batch 3770 of 25000, training loss: 0.130346\n",
      "Train Epoch: 0, mini-batch 3780 of 25000, training loss: 0.144949\n",
      "Train Epoch: 0, mini-batch 3790 of 25000, training loss: 1.811694\n",
      "Train Epoch: 0, mini-batch 3800 of 25000, training loss: 0.872281\n",
      "Train Epoch: 0, mini-batch 3810 of 25000, training loss: 0.165289\n",
      "Train Epoch: 0, mini-batch 3820 of 25000, training loss: 0.257738\n",
      "Train Epoch: 0, mini-batch 3830 of 25000, training loss: 0.032666\n",
      "Train Epoch: 0, mini-batch 3840 of 25000, training loss: 0.640953\n",
      "Train Epoch: 0, mini-batch 3850 of 25000, training loss: 0.151900\n",
      "Train Epoch: 0, mini-batch 3860 of 25000, training loss: 0.563896\n",
      "Train Epoch: 0, mini-batch 3870 of 25000, training loss: 0.717402\n",
      "Train Epoch: 0, mini-batch 3880 of 25000, training loss: 0.021739\n",
      "Train Epoch: 0, mini-batch 3890 of 25000, training loss: 0.202039\n",
      "Train Epoch: 0, mini-batch 3900 of 25000, training loss: 0.023497\n",
      "Train Epoch: 0, mini-batch 3910 of 25000, training loss: 1.386580\n",
      "Train Epoch: 0, mini-batch 3920 of 25000, training loss: 0.200498\n",
      "Train Epoch: 0, mini-batch 3930 of 25000, training loss: 0.952263\n",
      "Train Epoch: 0, mini-batch 3940 of 25000, training loss: 0.056748\n",
      "Train Epoch: 0, mini-batch 3950 of 25000, training loss: 0.516109\n",
      "Train Epoch: 0, mini-batch 3960 of 25000, training loss: 0.075259\n",
      "Train Epoch: 0, mini-batch 3970 of 25000, training loss: 0.022847\n",
      "Train Epoch: 0, mini-batch 3980 of 25000, training loss: 0.064802\n",
      "Train Epoch: 0, mini-batch 3990 of 25000, training loss: 1.640363\n",
      "Train Epoch: 0, mini-batch 4000 of 25000, training loss: 0.797232\n",
      "Train Epoch: 0, mini-batch 4010 of 25000, training loss: 0.797780\n",
      "Train Epoch: 0, mini-batch 4020 of 25000, training loss: 0.217676\n",
      "Train Epoch: 0, mini-batch 4030 of 25000, training loss: 0.009530\n",
      "Train Epoch: 0, mini-batch 4040 of 25000, training loss: 0.067749\n",
      "Train Epoch: 0, mini-batch 4050 of 25000, training loss: 0.040737\n",
      "Train Epoch: 0, mini-batch 4060 of 25000, training loss: 0.485678\n",
      "Train Epoch: 0, mini-batch 4070 of 25000, training loss: 0.409683\n",
      "Train Epoch: 0, mini-batch 4080 of 25000, training loss: 0.071403\n",
      "Train Epoch: 0, mini-batch 4090 of 25000, training loss: 2.225846\n",
      "Train Epoch: 0, mini-batch 4100 of 25000, training loss: 0.110350\n",
      "Train Epoch: 0, mini-batch 4110 of 25000, training loss: 1.162465\n",
      "Train Epoch: 0, mini-batch 4120 of 25000, training loss: 0.092784\n",
      "Train Epoch: 0, mini-batch 4130 of 25000, training loss: 0.359605\n",
      "Train Epoch: 0, mini-batch 4140 of 25000, training loss: 0.054386\n",
      "Train Epoch: 0, mini-batch 4150 of 25000, training loss: 0.090457\n",
      "Train Epoch: 0, mini-batch 4160 of 25000, training loss: 0.015027\n",
      "Train Epoch: 0, mini-batch 4170 of 25000, training loss: 0.591548\n",
      "Train Epoch: 0, mini-batch 4180 of 25000, training loss: 0.029357\n",
      "Train Epoch: 0, mini-batch 4190 of 25000, training loss: 1.423258\n",
      "Train Epoch: 0, mini-batch 4200 of 25000, training loss: 0.075969\n",
      "Train Epoch: 0, mini-batch 4210 of 25000, training loss: 0.000690\n",
      "Train Epoch: 0, mini-batch 4220 of 25000, training loss: 0.184761\n",
      "Train Epoch: 0, mini-batch 4230 of 25000, training loss: 0.411989\n",
      "Train Epoch: 0, mini-batch 4240 of 25000, training loss: 0.502716\n",
      "Train Epoch: 0, mini-batch 4250 of 25000, training loss: 0.126291\n",
      "Train Epoch: 0, mini-batch 4260 of 25000, training loss: 0.046672\n",
      "Train Epoch: 0, mini-batch 4270 of 25000, training loss: 0.570554\n",
      "Train Epoch: 0, mini-batch 4280 of 25000, training loss: 0.113426\n",
      "Train Epoch: 0, mini-batch 4290 of 25000, training loss: 0.160384\n",
      "Train Epoch: 0, mini-batch 4300 of 25000, training loss: 0.073626\n",
      "Train Epoch: 0, mini-batch 4310 of 25000, training loss: 0.145072\n",
      "Train Epoch: 0, mini-batch 4320 of 25000, training loss: 0.019979\n",
      "Train Epoch: 0, mini-batch 4330 of 25000, training loss: 0.880137\n",
      "Train Epoch: 0, mini-batch 4340 of 25000, training loss: 1.898659\n",
      "Train Epoch: 0, mini-batch 4350 of 25000, training loss: 0.059724\n",
      "Train Epoch: 0, mini-batch 4360 of 25000, training loss: 0.076767\n",
      "Train Epoch: 0, mini-batch 4370 of 25000, training loss: 0.633335\n",
      "Train Epoch: 0, mini-batch 4380 of 25000, training loss: 0.526971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, mini-batch 4390 of 25000, training loss: 0.137279\n",
      "Train Epoch: 0, mini-batch 4400 of 25000, training loss: 0.536962\n",
      "Train Epoch: 0, mini-batch 4410 of 25000, training loss: 0.019898\n",
      "Train Epoch: 0, mini-batch 4420 of 25000, training loss: 0.006939\n",
      "Train Epoch: 0, mini-batch 4430 of 25000, training loss: 0.003244\n",
      "Train Epoch: 0, mini-batch 4440 of 25000, training loss: 0.069077\n",
      "Train Epoch: 0, mini-batch 4450 of 25000, training loss: 0.007658\n",
      "Train Epoch: 0, mini-batch 4460 of 25000, training loss: 1.245637\n",
      "Train Epoch: 0, mini-batch 4470 of 25000, training loss: 0.005125\n",
      "Train Epoch: 0, mini-batch 4480 of 25000, training loss: 0.412028\n",
      "Train Epoch: 0, mini-batch 4490 of 25000, training loss: 0.014296\n",
      "Train Epoch: 0, mini-batch 4500 of 25000, training loss: 0.000758\n",
      "Train Epoch: 0, mini-batch 4510 of 25000, training loss: 0.485406\n",
      "Train Epoch: 0, mini-batch 4520 of 25000, training loss: 0.237051\n",
      "Train Epoch: 0, mini-batch 4530 of 25000, training loss: 0.241406\n",
      "Train Epoch: 0, mini-batch 4540 of 25000, training loss: 4.178461\n",
      "Train Epoch: 0, mini-batch 4550 of 25000, training loss: 1.712239\n",
      "Train Epoch: 0, mini-batch 4560 of 25000, training loss: 0.200785\n",
      "Train Epoch: 0, mini-batch 4570 of 25000, training loss: 0.116878\n",
      "Train Epoch: 0, mini-batch 4580 of 25000, training loss: 0.668899\n",
      "Train Epoch: 0, mini-batch 4590 of 25000, training loss: 0.594378\n",
      "Train Epoch: 0, mini-batch 4600 of 25000, training loss: 0.011186\n",
      "Train Epoch: 0, mini-batch 4610 of 25000, training loss: 0.002831\n",
      "Train Epoch: 0, mini-batch 4620 of 25000, training loss: 0.076534\n",
      "Train Epoch: 0, mini-batch 4630 of 25000, training loss: 0.179800\n",
      "Train Epoch: 0, mini-batch 4640 of 25000, training loss: 1.150519\n",
      "Train Epoch: 0, mini-batch 4650 of 25000, training loss: 0.229993\n",
      "Train Epoch: 0, mini-batch 4660 of 25000, training loss: 0.045217\n",
      "Train Epoch: 0, mini-batch 4670 of 25000, training loss: 0.092842\n",
      "Train Epoch: 0, mini-batch 4680 of 25000, training loss: 0.304769\n",
      "Train Epoch: 0, mini-batch 4690 of 25000, training loss: 0.179956\n",
      "Train Epoch: 0, mini-batch 4700 of 25000, training loss: 0.074571\n",
      "Train Epoch: 0, mini-batch 4710 of 25000, training loss: 0.501476\n",
      "Train Epoch: 0, mini-batch 4720 of 25000, training loss: 0.519955\n",
      "Train Epoch: 0, mini-batch 4730 of 25000, training loss: 1.775169\n",
      "Train Epoch: 0, mini-batch 4740 of 25000, training loss: 0.096563\n",
      "Train Epoch: 0, mini-batch 4750 of 25000, training loss: 0.160049\n",
      "Train Epoch: 0, mini-batch 4760 of 25000, training loss: 0.017348\n",
      "Train Epoch: 0, mini-batch 4770 of 25000, training loss: 0.038941\n",
      "Train Epoch: 0, mini-batch 4780 of 25000, training loss: 0.190627\n",
      "Train Epoch: 0, mini-batch 4790 of 25000, training loss: 2.254639\n",
      "Train Epoch: 0, mini-batch 4800 of 25000, training loss: 0.149324\n",
      "Train Epoch: 0, mini-batch 4810 of 25000, training loss: 0.092296\n",
      "Train Epoch: 0, mini-batch 4820 of 25000, training loss: 0.172662\n",
      "Train Epoch: 0, mini-batch 4830 of 25000, training loss: 0.782315\n",
      "Train Epoch: 0, mini-batch 4840 of 25000, training loss: 0.316293\n",
      "Train Epoch: 0, mini-batch 4850 of 25000, training loss: 0.116408\n",
      "Train Epoch: 0, mini-batch 4860 of 25000, training loss: 0.130608\n",
      "Train Epoch: 0, mini-batch 4870 of 25000, training loss: 0.417313\n",
      "Train Epoch: 0, mini-batch 4880 of 25000, training loss: 0.539043\n",
      "Train Epoch: 0, mini-batch 4890 of 25000, training loss: 0.338910\n",
      "Train Epoch: 0, mini-batch 4900 of 25000, training loss: 0.091508\n",
      "Train Epoch: 0, mini-batch 4910 of 25000, training loss: 0.835340\n",
      "Train Epoch: 0, mini-batch 4920 of 25000, training loss: 0.027929\n",
      "Train Epoch: 0, mini-batch 4930 of 25000, training loss: 0.048593\n",
      "Train Epoch: 0, mini-batch 4940 of 25000, training loss: 0.507728\n",
      "Train Epoch: 0, mini-batch 4950 of 25000, training loss: 2.644018\n",
      "Train Epoch: 0, mini-batch 4960 of 25000, training loss: 0.129662\n",
      "Train Epoch: 0, mini-batch 4970 of 25000, training loss: 0.196135\n",
      "Train Epoch: 0, mini-batch 4980 of 25000, training loss: 0.051304\n",
      "Train Epoch: 0, mini-batch 4990 of 25000, training loss: 1.107980\n",
      "Train Epoch: 0, mini-batch 5000 of 25000, training loss: 0.112421\n",
      "Train Epoch: 0, mini-batch 5010 of 25000, training loss: 0.281170\n",
      "Train Epoch: 0, mini-batch 5020 of 25000, training loss: 0.033736\n",
      "Train Epoch: 0, mini-batch 5030 of 25000, training loss: 0.614515\n",
      "Train Epoch: 0, mini-batch 5040 of 25000, training loss: 0.008331\n",
      "Train Epoch: 0, mini-batch 5050 of 25000, training loss: 0.076775\n",
      "Train Epoch: 0, mini-batch 5060 of 25000, training loss: 0.003595\n",
      "Train Epoch: 0, mini-batch 5070 of 25000, training loss: 0.117952\n",
      "Train Epoch: 0, mini-batch 5080 of 25000, training loss: 0.034028\n",
      "Train Epoch: 0, mini-batch 5090 of 25000, training loss: 0.056904\n",
      "Train Epoch: 0, mini-batch 5100 of 25000, training loss: 1.559391\n",
      "Train Epoch: 0, mini-batch 5110 of 25000, training loss: 1.700066\n",
      "Train Epoch: 0, mini-batch 5120 of 25000, training loss: 0.089952\n",
      "Train Epoch: 0, mini-batch 5130 of 25000, training loss: 0.184637\n",
      "Train Epoch: 0, mini-batch 5140 of 25000, training loss: 0.261373\n",
      "Train Epoch: 0, mini-batch 5150 of 25000, training loss: 0.042168\n",
      "Train Epoch: 0, mini-batch 5160 of 25000, training loss: 0.030930\n",
      "Train Epoch: 0, mini-batch 5170 of 25000, training loss: 0.860566\n",
      "Train Epoch: 0, mini-batch 5180 of 25000, training loss: 0.050977\n",
      "Train Epoch: 0, mini-batch 5190 of 25000, training loss: 0.064055\n",
      "Train Epoch: 0, mini-batch 5200 of 25000, training loss: 0.738868\n",
      "Train Epoch: 0, mini-batch 5210 of 25000, training loss: 0.499646\n",
      "Train Epoch: 0, mini-batch 5220 of 25000, training loss: 0.435871\n",
      "Train Epoch: 0, mini-batch 5230 of 25000, training loss: 0.009056\n",
      "Train Epoch: 0, mini-batch 5240 of 25000, training loss: 0.102736\n",
      "Train Epoch: 0, mini-batch 5250 of 25000, training loss: 0.390374\n",
      "Train Epoch: 0, mini-batch 5260 of 25000, training loss: 0.079433\n",
      "Train Epoch: 0, mini-batch 5270 of 25000, training loss: 0.391329\n",
      "Train Epoch: 0, mini-batch 5280 of 25000, training loss: 0.037969\n",
      "Train Epoch: 0, mini-batch 5290 of 25000, training loss: 0.161814\n",
      "Train Epoch: 0, mini-batch 5300 of 25000, training loss: 0.164266\n",
      "Train Epoch: 0, mini-batch 5310 of 25000, training loss: 0.022159\n",
      "Train Epoch: 0, mini-batch 5320 of 25000, training loss: 0.006423\n",
      "Train Epoch: 0, mini-batch 5330 of 25000, training loss: 0.053940\n",
      "Train Epoch: 0, mini-batch 5340 of 25000, training loss: 1.333614\n",
      "Train Epoch: 0, mini-batch 5350 of 25000, training loss: 0.510633\n",
      "Train Epoch: 0, mini-batch 5360 of 25000, training loss: 0.999817\n",
      "Train Epoch: 0, mini-batch 5370 of 25000, training loss: 0.086588\n",
      "Train Epoch: 0, mini-batch 5380 of 25000, training loss: 0.021655\n",
      "Train Epoch: 0, mini-batch 5390 of 25000, training loss: 0.053272\n",
      "Train Epoch: 0, mini-batch 5400 of 25000, training loss: 0.272774\n",
      "Train Epoch: 0, mini-batch 5410 of 25000, training loss: 0.112912\n",
      "Train Epoch: 0, mini-batch 5420 of 25000, training loss: 1.041394\n",
      "Train Epoch: 0, mini-batch 5430 of 25000, training loss: 0.054163\n",
      "Train Epoch: 0, mini-batch 5440 of 25000, training loss: 0.143818\n",
      "Train Epoch: 0, mini-batch 5450 of 25000, training loss: 0.313077\n",
      "Train Epoch: 0, mini-batch 5460 of 25000, training loss: 1.962332\n",
      "Train Epoch: 0, mini-batch 5470 of 25000, training loss: 0.467337\n",
      "Train Epoch: 0, mini-batch 5480 of 25000, training loss: 0.017998\n",
      "Train Epoch: 0, mini-batch 5490 of 25000, training loss: 0.029729\n",
      "Train Epoch: 0, mini-batch 5500 of 25000, training loss: 0.019518\n",
      "Train Epoch: 0, mini-batch 5510 of 25000, training loss: 0.048423\n",
      "Train Epoch: 0, mini-batch 5520 of 25000, training loss: 0.072544\n",
      "Train Epoch: 0, mini-batch 5530 of 25000, training loss: 0.072624\n",
      "Train Epoch: 0, mini-batch 5540 of 25000, training loss: 0.003375\n",
      "Train Epoch: 0, mini-batch 5550 of 25000, training loss: 0.074300\n",
      "Train Epoch: 0, mini-batch 5560 of 25000, training loss: 0.099928\n",
      "Train Epoch: 0, mini-batch 5570 of 25000, training loss: 0.426929\n",
      "Train Epoch: 0, mini-batch 5580 of 25000, training loss: 0.045134\n",
      "Train Epoch: 0, mini-batch 5590 of 25000, training loss: 0.063968\n",
      "Train Epoch: 0, mini-batch 5600 of 25000, training loss: 0.113938\n",
      "Train Epoch: 0, mini-batch 5610 of 25000, training loss: 1.083324\n",
      "Train Epoch: 0, mini-batch 5620 of 25000, training loss: 0.120848\n",
      "Train Epoch: 0, mini-batch 5630 of 25000, training loss: 0.065035\n",
      "Train Epoch: 0, mini-batch 5640 of 25000, training loss: 0.023460\n",
      "Train Epoch: 0, mini-batch 5650 of 25000, training loss: 0.039534\n",
      "Train Epoch: 0, mini-batch 5660 of 25000, training loss: 0.008305\n",
      "Train Epoch: 0, mini-batch 5670 of 25000, training loss: 0.081846\n",
      "Train Epoch: 0, mini-batch 5680 of 25000, training loss: 0.130601\n",
      "Train Epoch: 0, mini-batch 5690 of 25000, training loss: 0.373856\n",
      "Train Epoch: 0, mini-batch 5700 of 25000, training loss: 0.090181\n",
      "Train Epoch: 0, mini-batch 5710 of 25000, training loss: 0.604363\n",
      "Train Epoch: 0, mini-batch 5720 of 25000, training loss: 0.115374\n",
      "Train Epoch: 0, mini-batch 5730 of 25000, training loss: 0.050224\n",
      "Train Epoch: 0, mini-batch 5740 of 25000, training loss: 0.088934\n",
      "Train Epoch: 0, mini-batch 5750 of 25000, training loss: 0.033568\n",
      "Train Epoch: 0, mini-batch 5760 of 25000, training loss: 0.011433\n",
      "Train Epoch: 0, mini-batch 5770 of 25000, training loss: 0.056066\n",
      "Train Epoch: 0, mini-batch 5780 of 25000, training loss: 0.040270\n",
      "Train Epoch: 0, mini-batch 5790 of 25000, training loss: 0.239253\n",
      "Train Epoch: 0, mini-batch 5800 of 25000, training loss: 0.037529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, mini-batch 5810 of 25000, training loss: 0.044221\n",
      "Train Epoch: 0, mini-batch 5820 of 25000, training loss: 0.259877\n",
      "Train Epoch: 0, mini-batch 5830 of 25000, training loss: 0.141588\n",
      "Train Epoch: 0, mini-batch 5840 of 25000, training loss: 0.139451\n",
      "Train Epoch: 0, mini-batch 5850 of 25000, training loss: 0.239557\n",
      "Train Epoch: 0, mini-batch 5860 of 25000, training loss: 0.247746\n",
      "Train Epoch: 0, mini-batch 5870 of 25000, training loss: 0.408890\n",
      "Train Epoch: 0, mini-batch 5880 of 25000, training loss: 0.565888\n",
      "Train Epoch: 0, mini-batch 5890 of 25000, training loss: 0.037637\n",
      "Train Epoch: 0, mini-batch 5900 of 25000, training loss: 0.006358\n",
      "Train Epoch: 0, mini-batch 5910 of 25000, training loss: 0.208833\n",
      "Train Epoch: 0, mini-batch 5920 of 25000, training loss: 0.038720\n",
      "Train Epoch: 0, mini-batch 5930 of 25000, training loss: 0.166565\n",
      "Train Epoch: 0, mini-batch 5940 of 25000, training loss: 1.087361\n",
      "Train Epoch: 0, mini-batch 5950 of 25000, training loss: 0.000593\n",
      "Train Epoch: 0, mini-batch 5960 of 25000, training loss: 0.947192\n",
      "Train Epoch: 0, mini-batch 5970 of 25000, training loss: 0.058717\n",
      "Train Epoch: 0, mini-batch 5980 of 25000, training loss: 0.030108\n",
      "Train Epoch: 0, mini-batch 5990 of 25000, training loss: 0.127106\n",
      "Train Epoch: 0, mini-batch 6000 of 25000, training loss: 0.011295\n",
      "Train Epoch: 0, mini-batch 6010 of 25000, training loss: 0.176928\n",
      "Train Epoch: 0, mini-batch 6020 of 25000, training loss: 0.295354\n",
      "Train Epoch: 0, mini-batch 6030 of 25000, training loss: 3.740569\n",
      "Train Epoch: 0, mini-batch 6040 of 25000, training loss: 0.232661\n",
      "Train Epoch: 0, mini-batch 6050 of 25000, training loss: 0.884697\n",
      "Train Epoch: 0, mini-batch 6060 of 25000, training loss: 0.015424\n",
      "Train Epoch: 0, mini-batch 6070 of 25000, training loss: 0.197509\n",
      "Train Epoch: 0, mini-batch 6080 of 25000, training loss: 0.648869\n",
      "Train Epoch: 0, mini-batch 6090 of 25000, training loss: 0.392778\n",
      "Train Epoch: 0, mini-batch 6100 of 25000, training loss: 0.075115\n",
      "Train Epoch: 0, mini-batch 6110 of 25000, training loss: 0.029641\n",
      "Train Epoch: 0, mini-batch 6120 of 25000, training loss: 0.073507\n",
      "Train Epoch: 0, mini-batch 6130 of 25000, training loss: 0.035153\n",
      "Train Epoch: 0, mini-batch 6140 of 25000, training loss: 0.033611\n",
      "Train Epoch: 0, mini-batch 6150 of 25000, training loss: 1.711887\n",
      "Train Epoch: 0, mini-batch 6160 of 25000, training loss: 0.815970\n",
      "Train Epoch: 0, mini-batch 6170 of 25000, training loss: 0.040560\n",
      "Train Epoch: 0, mini-batch 6180 of 25000, training loss: 0.303453\n",
      "Train Epoch: 0, mini-batch 6190 of 25000, training loss: 0.022388\n",
      "Train Epoch: 0, mini-batch 6200 of 25000, training loss: 0.035423\n",
      "Train Epoch: 0, mini-batch 6210 of 25000, training loss: 0.079657\n",
      "Train Epoch: 0, mini-batch 6220 of 25000, training loss: 0.047207\n",
      "Train Epoch: 0, mini-batch 6230 of 25000, training loss: 0.020010\n",
      "Train Epoch: 0, mini-batch 6240 of 25000, training loss: 0.057837\n",
      "Train Epoch: 0, mini-batch 6250 of 25000, training loss: 1.032039\n",
      "Train Epoch: 0, mini-batch 6260 of 25000, training loss: 0.037150\n",
      "Train Epoch: 0, mini-batch 6270 of 25000, training loss: 0.136402\n",
      "Train Epoch: 0, mini-batch 6280 of 25000, training loss: 0.076338\n",
      "Train Epoch: 0, mini-batch 6290 of 25000, training loss: 0.059853\n",
      "Train Epoch: 0, mini-batch 6300 of 25000, training loss: 0.214809\n",
      "Train Epoch: 0, mini-batch 6310 of 25000, training loss: 0.012964\n",
      "Train Epoch: 0, mini-batch 6320 of 25000, training loss: 0.069316\n",
      "Train Epoch: 0, mini-batch 6330 of 25000, training loss: 1.103394\n",
      "Train Epoch: 0, mini-batch 6340 of 25000, training loss: 0.051195\n",
      "Train Epoch: 0, mini-batch 6350 of 25000, training loss: 0.073734\n",
      "Train Epoch: 0, mini-batch 6360 of 25000, training loss: 0.143069\n",
      "Train Epoch: 0, mini-batch 6370 of 25000, training loss: 0.115412\n",
      "Train Epoch: 0, mini-batch 6380 of 25000, training loss: 0.067488\n",
      "Train Epoch: 0, mini-batch 6390 of 25000, training loss: 1.630473\n",
      "Train Epoch: 0, mini-batch 6400 of 25000, training loss: 0.012796\n",
      "Train Epoch: 0, mini-batch 6410 of 25000, training loss: 0.312764\n",
      "Train Epoch: 0, mini-batch 6420 of 25000, training loss: 0.079365\n",
      "Train Epoch: 0, mini-batch 6430 of 25000, training loss: 0.008137\n",
      "Train Epoch: 0, mini-batch 6440 of 25000, training loss: 0.005922\n",
      "Train Epoch: 0, mini-batch 6450 of 25000, training loss: 0.004694\n",
      "Train Epoch: 0, mini-batch 6460 of 25000, training loss: 0.234591\n",
      "Train Epoch: 0, mini-batch 6470 of 25000, training loss: 0.193208\n",
      "Train Epoch: 0, mini-batch 6480 of 25000, training loss: 1.261642\n",
      "Train Epoch: 0, mini-batch 6490 of 25000, training loss: 0.018627\n",
      "Train Epoch: 0, mini-batch 6500 of 25000, training loss: 0.003709\n",
      "Train Epoch: 0, mini-batch 6510 of 25000, training loss: 0.688304\n",
      "Train Epoch: 0, mini-batch 6520 of 25000, training loss: 0.643371\n",
      "Train Epoch: 0, mini-batch 6530 of 25000, training loss: 0.003834\n",
      "Train Epoch: 0, mini-batch 6540 of 25000, training loss: 0.359744\n",
      "Train Epoch: 0, mini-batch 6550 of 25000, training loss: 0.370996\n",
      "Train Epoch: 0, mini-batch 6560 of 25000, training loss: 0.006597\n",
      "Train Epoch: 0, mini-batch 6570 of 25000, training loss: 0.631328\n",
      "Train Epoch: 0, mini-batch 6580 of 25000, training loss: 0.150826\n",
      "Train Epoch: 0, mini-batch 6590 of 25000, training loss: 0.040139\n",
      "Train Epoch: 0, mini-batch 6600 of 25000, training loss: 0.252906\n",
      "Train Epoch: 0, mini-batch 6610 of 25000, training loss: 0.123324\n",
      "Train Epoch: 0, mini-batch 6620 of 25000, training loss: 0.003715\n",
      "Train Epoch: 0, mini-batch 6630 of 25000, training loss: 0.635349\n",
      "Train Epoch: 0, mini-batch 6640 of 25000, training loss: 0.043910\n",
      "Train Epoch: 0, mini-batch 6650 of 25000, training loss: 0.057740\n",
      "Train Epoch: 0, mini-batch 6660 of 25000, training loss: 0.682610\n",
      "Train Epoch: 0, mini-batch 6670 of 25000, training loss: 0.142203\n",
      "Train Epoch: 0, mini-batch 6680 of 25000, training loss: 0.299213\n",
      "Train Epoch: 0, mini-batch 6690 of 25000, training loss: 1.678984\n",
      "Train Epoch: 0, mini-batch 6700 of 25000, training loss: 0.028275\n",
      "Train Epoch: 0, mini-batch 6710 of 25000, training loss: 0.054415\n",
      "Train Epoch: 0, mini-batch 6720 of 25000, training loss: 0.022297\n",
      "Train Epoch: 0, mini-batch 6730 of 25000, training loss: 0.013005\n",
      "Train Epoch: 0, mini-batch 6740 of 25000, training loss: 0.264912\n",
      "Train Epoch: 0, mini-batch 6750 of 25000, training loss: 0.025834\n",
      "Train Epoch: 0, mini-batch 6760 of 25000, training loss: 0.049696\n",
      "Train Epoch: 0, mini-batch 6770 of 25000, training loss: 0.000294\n",
      "Train Epoch: 0, mini-batch 6780 of 25000, training loss: 0.020070\n",
      "Train Epoch: 0, mini-batch 6790 of 25000, training loss: 0.015703\n",
      "Train Epoch: 0, mini-batch 6800 of 25000, training loss: 0.004783\n",
      "Train Epoch: 0, mini-batch 6810 of 25000, training loss: 0.057380\n",
      "Train Epoch: 0, mini-batch 6820 of 25000, training loss: 0.036350\n",
      "Train Epoch: 0, mini-batch 6830 of 25000, training loss: 0.010512\n",
      "Train Epoch: 0, mini-batch 6840 of 25000, training loss: 0.014921\n",
      "Train Epoch: 0, mini-batch 6850 of 25000, training loss: 0.037615\n",
      "Train Epoch: 0, mini-batch 6860 of 25000, training loss: 0.157621\n",
      "Train Epoch: 0, mini-batch 6870 of 25000, training loss: 1.368108\n",
      "Train Epoch: 0, mini-batch 6880 of 25000, training loss: 0.046048\n",
      "Train Epoch: 0, mini-batch 6890 of 25000, training loss: 0.059396\n",
      "Train Epoch: 0, mini-batch 6900 of 25000, training loss: 0.013552\n",
      "Train Epoch: 0, mini-batch 6910 of 25000, training loss: 0.377018\n",
      "Train Epoch: 0, mini-batch 6920 of 25000, training loss: 0.018965\n",
      "Train Epoch: 0, mini-batch 6930 of 25000, training loss: 1.292928\n",
      "Train Epoch: 0, mini-batch 6940 of 25000, training loss: 1.178697\n",
      "Train Epoch: 0, mini-batch 6950 of 25000, training loss: 0.024756\n",
      "Train Epoch: 0, mini-batch 6960 of 25000, training loss: 0.259326\n",
      "Train Epoch: 0, mini-batch 6970 of 25000, training loss: 0.001705\n",
      "Train Epoch: 0, mini-batch 6980 of 25000, training loss: 0.165623\n",
      "Train Epoch: 0, mini-batch 6990 of 25000, training loss: 0.011497\n",
      "Train Epoch: 0, mini-batch 7000 of 25000, training loss: 0.311110\n",
      "Train Epoch: 0, mini-batch 7010 of 25000, training loss: 0.180942\n",
      "Train Epoch: 0, mini-batch 7020 of 25000, training loss: 1.492101\n",
      "Train Epoch: 0, mini-batch 7030 of 25000, training loss: 1.043594\n",
      "Train Epoch: 0, mini-batch 7040 of 25000, training loss: 0.460529\n",
      "Train Epoch: 0, mini-batch 7050 of 25000, training loss: 0.094009\n",
      "Train Epoch: 0, mini-batch 7060 of 25000, training loss: 0.383154\n",
      "Train Epoch: 0, mini-batch 7070 of 25000, training loss: 0.042263\n",
      "Train Epoch: 0, mini-batch 7080 of 25000, training loss: 0.160668\n",
      "Train Epoch: 0, mini-batch 7090 of 25000, training loss: 1.338556\n",
      "Train Epoch: 0, mini-batch 7100 of 25000, training loss: 0.020581\n",
      "Train Epoch: 0, mini-batch 7110 of 25000, training loss: 0.056142\n",
      "Train Epoch: 0, mini-batch 7120 of 25000, training loss: 0.211484\n",
      "Train Epoch: 0, mini-batch 7130 of 25000, training loss: 0.188331\n",
      "Train Epoch: 0, mini-batch 7140 of 25000, training loss: 0.900662\n",
      "Train Epoch: 0, mini-batch 7150 of 25000, training loss: 0.236946\n",
      "Train Epoch: 0, mini-batch 7160 of 25000, training loss: 0.888573\n",
      "Train Epoch: 0, mini-batch 7170 of 25000, training loss: 0.501405\n",
      "Train Epoch: 0, mini-batch 7180 of 25000, training loss: 0.778237\n",
      "Train Epoch: 0, mini-batch 7190 of 25000, training loss: 0.053180\n",
      "Train Epoch: 0, mini-batch 7200 of 25000, training loss: 0.024742\n",
      "Train Epoch: 0, mini-batch 7210 of 25000, training loss: 0.269109\n",
      "Train Epoch: 0, mini-batch 7220 of 25000, training loss: 0.460902\n",
      "Train Epoch: 0, mini-batch 7230 of 25000, training loss: 0.009965\n",
      "Train Epoch: 0, mini-batch 7240 of 25000, training loss: 0.152705\n",
      "Train Epoch: 0, mini-batch 7250 of 25000, training loss: 0.055164\n",
      "Train Epoch: 0, mini-batch 7260 of 25000, training loss: 0.261004\n",
      "Train Epoch: 0, mini-batch 7270 of 25000, training loss: 0.031627\n",
      "Train Epoch: 0, mini-batch 7280 of 25000, training loss: 0.493478\n",
      "Train Epoch: 0, mini-batch 7290 of 25000, training loss: 0.015397\n",
      "Train Epoch: 0, mini-batch 7300 of 25000, training loss: 0.008658\n",
      "Train Epoch: 0, mini-batch 7310 of 25000, training loss: 1.523115\n",
      "Train Epoch: 0, mini-batch 7320 of 25000, training loss: 0.480983\n",
      "Train Epoch: 0, mini-batch 7330 of 25000, training loss: 1.172746\n",
      "Train Epoch: 0, mini-batch 7340 of 25000, training loss: 4.909890\n",
      "Train Epoch: 0, mini-batch 7350 of 25000, training loss: 0.798759\n",
      "Train Epoch: 0, mini-batch 7360 of 25000, training loss: 0.283501\n",
      "Train Epoch: 0, mini-batch 7370 of 25000, training loss: 0.040502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, mini-batch 7380 of 25000, training loss: 0.001470\n",
      "Train Epoch: 0, mini-batch 7390 of 25000, training loss: 0.047409\n",
      "Train Epoch: 0, mini-batch 7400 of 25000, training loss: 0.088814\n",
      "Train Epoch: 0, mini-batch 7410 of 25000, training loss: 0.496978\n",
      "Train Epoch: 0, mini-batch 7420 of 25000, training loss: 0.114330\n",
      "Train Epoch: 0, mini-batch 7430 of 25000, training loss: 0.114552\n",
      "Train Epoch: 0, mini-batch 7440 of 25000, training loss: 0.055324\n",
      "Train Epoch: 0, mini-batch 7450 of 25000, training loss: 0.617035\n",
      "Train Epoch: 0, mini-batch 7460 of 25000, training loss: 0.139774\n",
      "Train Epoch: 0, mini-batch 7470 of 25000, training loss: 1.972763\n",
      "Train Epoch: 0, mini-batch 7480 of 25000, training loss: 0.408044\n",
      "Train Epoch: 0, mini-batch 7490 of 25000, training loss: 0.405534\n",
      "Train Epoch: 0, mini-batch 7500 of 25000, training loss: 0.456997\n",
      "Train Epoch: 0, mini-batch 7510 of 25000, training loss: 0.254052\n",
      "Train Epoch: 0, mini-batch 7520 of 25000, training loss: 0.064884\n",
      "Train Epoch: 0, mini-batch 7530 of 25000, training loss: 0.468812\n",
      "Train Epoch: 0, mini-batch 7540 of 25000, training loss: 0.457831\n",
      "Train Epoch: 0, mini-batch 7550 of 25000, training loss: 0.056473\n",
      "Train Epoch: 0, mini-batch 7560 of 25000, training loss: 0.534617\n",
      "Train Epoch: 0, mini-batch 7570 of 25000, training loss: 0.078690\n",
      "Train Epoch: 0, mini-batch 7580 of 25000, training loss: 0.004602\n",
      "Train Epoch: 0, mini-batch 7590 of 25000, training loss: 0.473834\n",
      "Train Epoch: 0, mini-batch 7600 of 25000, training loss: 0.038629\n",
      "Train Epoch: 0, mini-batch 7610 of 25000, training loss: 0.260048\n",
      "Train Epoch: 0, mini-batch 7620 of 25000, training loss: 0.452336\n",
      "Train Epoch: 0, mini-batch 7630 of 25000, training loss: 1.178030\n",
      "Train Epoch: 0, mini-batch 7640 of 25000, training loss: 0.993926\n",
      "Train Epoch: 0, mini-batch 7650 of 25000, training loss: 0.981106\n",
      "Train Epoch: 0, mini-batch 7660 of 25000, training loss: 0.080489\n",
      "Train Epoch: 0, mini-batch 7670 of 25000, training loss: 0.002505\n",
      "Train Epoch: 0, mini-batch 7680 of 25000, training loss: 0.400288\n",
      "Train Epoch: 0, mini-batch 7690 of 25000, training loss: 0.194688\n",
      "Train Epoch: 0, mini-batch 7700 of 25000, training loss: 0.004360\n",
      "Train Epoch: 0, mini-batch 7710 of 25000, training loss: 2.216873\n",
      "Train Epoch: 0, mini-batch 7720 of 25000, training loss: 2.738574\n",
      "Train Epoch: 0, mini-batch 7730 of 25000, training loss: 0.208076\n",
      "Train Epoch: 0, mini-batch 7740 of 25000, training loss: 1.343084\n",
      "Train Epoch: 0, mini-batch 7750 of 25000, training loss: 0.045513\n",
      "Train Epoch: 0, mini-batch 7760 of 25000, training loss: 0.839972\n",
      "Train Epoch: 0, mini-batch 7770 of 25000, training loss: 0.096866\n",
      "Train Epoch: 0, mini-batch 7780 of 25000, training loss: 0.710873\n",
      "Train Epoch: 0, mini-batch 7790 of 25000, training loss: 1.186801\n",
      "Train Epoch: 0, mini-batch 7800 of 25000, training loss: 0.190923\n",
      "Train Epoch: 0, mini-batch 7810 of 25000, training loss: 0.024555\n",
      "Train Epoch: 0, mini-batch 7820 of 25000, training loss: 0.093871\n",
      "Train Epoch: 0, mini-batch 7830 of 25000, training loss: 0.052984\n",
      "Train Epoch: 0, mini-batch 7840 of 25000, training loss: 0.007465\n",
      "Train Epoch: 0, mini-batch 7850 of 25000, training loss: 0.294105\n",
      "Train Epoch: 0, mini-batch 7860 of 25000, training loss: 0.161898\n",
      "Train Epoch: 0, mini-batch 7870 of 25000, training loss: 0.166007\n",
      "Train Epoch: 0, mini-batch 7880 of 25000, training loss: 0.038265\n",
      "Train Epoch: 0, mini-batch 7890 of 25000, training loss: 0.083429\n",
      "Train Epoch: 0, mini-batch 7900 of 25000, training loss: 0.851821\n",
      "Train Epoch: 0, mini-batch 7910 of 25000, training loss: 0.006586\n",
      "Train Epoch: 0, mini-batch 7920 of 25000, training loss: 0.002277\n",
      "Train Epoch: 0, mini-batch 7930 of 25000, training loss: 0.332389\n",
      "Train Epoch: 0, mini-batch 7940 of 25000, training loss: 0.011698\n",
      "Train Epoch: 0, mini-batch 7950 of 25000, training loss: 0.376424\n",
      "Train Epoch: 0, mini-batch 7960 of 25000, training loss: 0.018649\n",
      "Train Epoch: 0, mini-batch 7970 of 25000, training loss: 0.258203\n",
      "Train Epoch: 0, mini-batch 7980 of 25000, training loss: 0.134142\n",
      "Train Epoch: 0, mini-batch 7990 of 25000, training loss: 0.017120\n",
      "Train Epoch: 0, mini-batch 8000 of 25000, training loss: 0.089328\n",
      "Train Epoch: 0, mini-batch 8010 of 25000, training loss: 0.097191\n",
      "Train Epoch: 0, mini-batch 8020 of 25000, training loss: 0.033991\n",
      "Train Epoch: 0, mini-batch 8030 of 25000, training loss: 0.199381\n",
      "Train Epoch: 0, mini-batch 8040 of 25000, training loss: 0.303729\n",
      "Train Epoch: 0, mini-batch 8050 of 25000, training loss: 0.011067\n",
      "Train Epoch: 0, mini-batch 8060 of 25000, training loss: 0.526137\n",
      "Train Epoch: 0, mini-batch 8070 of 25000, training loss: 0.023419\n",
      "Train Epoch: 0, mini-batch 8080 of 25000, training loss: 0.484790\n",
      "Train Epoch: 0, mini-batch 8090 of 25000, training loss: 0.073315\n",
      "Train Epoch: 0, mini-batch 8100 of 25000, training loss: 0.855575\n",
      "Train Epoch: 0, mini-batch 8110 of 25000, training loss: 0.002118\n",
      "Train Epoch: 0, mini-batch 8120 of 25000, training loss: 1.077967\n",
      "Train Epoch: 0, mini-batch 8130 of 25000, training loss: 1.566742\n",
      "Train Epoch: 0, mini-batch 8140 of 25000, training loss: 0.077262\n",
      "Train Epoch: 0, mini-batch 8150 of 25000, training loss: 0.057445\n",
      "Train Epoch: 0, mini-batch 8160 of 25000, training loss: 2.513833\n",
      "Train Epoch: 0, mini-batch 8170 of 25000, training loss: 2.352978\n",
      "Train Epoch: 0, mini-batch 8180 of 25000, training loss: 0.315482\n",
      "Train Epoch: 0, mini-batch 8190 of 25000, training loss: 0.038923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a23aa749e47f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#nn.utils.clip_grad_norm(model.parameters(), 0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/CodasML/lib/python3.6/site-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "train_data_gen = zip(train_features, train_target)\n",
    "train_size = len(train_target)\n",
    "\n",
    "while epoch < epochs:\n",
    "    predictions = []\n",
    "    truth_values = []\n",
    "\n",
    "    for batch_idx, (xs, y) in enumerate(train_data_gen):\n",
    "        xs, y = torch.from_numpy(xs).float(), torch.FloatTensor([y])\n",
    "\n",
    "        y_pred = model(xs)\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        predictions.append(y_pred.cpu().data.numpy().ravel())\n",
    "        truth_values.append(y)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {}, mini-batch {} of {}, training loss: {:.6f}'.format(\n",
    "                epoch, batch_idx, train_size, loss.item()))\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls  # toggle 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test accuracy histories\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epoch, train_loss)\n",
    "plt.plot(epoch, test_loss)\n",
    "# plt.plot(no_reg['epoch'], no_reg['train_loss'])  # toggle 0\n",
    "# plt.plot(no_reg['epoch'], no_reg['test_loss'])  # toggle 0\n",
    "\n",
    "plt.legend(['Train loss', 'Test loss', 'Train no-reg', 'Test no-reg'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss score')\n",
    "\n",
    "# Get training and test accuracy histories\n",
    "train_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epoch, train_accuracy)\n",
    "plt.plot(epoch, test_accuracy)\n",
    "# plt.plot(no_reg['epoch'], no_reg['train_accuracy'])  # toggle 0\n",
    "# plt.plot(no_reg['epoch'], no_reg['test_accuracy'])  # toggle 0\n",
    "\n",
    "plt.legend(['Train accuracy', 'Test accuracy', 'Train no-reg', 'Test no-reg'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "\n",
    "no_reg = {                             # toggle 0\n",
    "    'epoch': epoch,                    # toggle 0\n",
    "    'train_loss': train_loss,          # toggle 0\n",
    "    'test_loss': test_loss,            # toggle 0\n",
    "    'train_accuracy': train_accuracy,  # toggle 0\n",
    "    'test_accuracy': test_accuracy,    # toggle 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup weights\n",
    "weights = network.layers[0].get_weights()[0]  # toggle 0\n",
    "# weights_L1 = network.layers[0].get_weights()[0]  # toggle 1\n",
    "# weights_L2 = network.layers[0].get_weights()[0]  # toggle 2\n",
    "# weights_max = network.layers[0].get_weights()[0]  # toggle 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you got to toggle `# toggle 3`, execute the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show weight distribution\n",
    "plt.hist((\n",
    "    weights.reshape(-1),\n",
    "    weights_L1.reshape(-1),\n",
    "    weights_L2.reshape(-1),\n",
    "    weights_max.reshape(-1),\n",
    "), 49, range=(-.5, .5), label=(\n",
    "    'No-reg',\n",
    "    'L1',\n",
    "    'L2',\n",
    "    'Max',\n",
    "))\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codas ML",
   "language": "python",
   "name": "codasml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
