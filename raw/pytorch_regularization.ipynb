{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start doing anything, I think it's important to understand for NLP, this is the intuitive process on what we are trying to do when we are processing our data in the IMDB dataset:\n",
    "1. Tokenization: break sentence into individual words\n",
    "    - Before: `\"PyTorch seems really easy to use!\"`\n",
    "    - After: `[\"PyTorch\", \"seems\", \"really\", \"easy\", \"to\", \"use\", \"!\"]`\n",
    "2. Building vocabulary: build an index of words associated with unique numbers\n",
    "    - Before: `[\"PyTorch\", \"seems\", \"really\", \"easy\", \"to\", \"use\", \"!\"]`\n",
    "    - After: `{\"Pytorch: 0, \"seems\": 1, \"really\": 2, ...}`\n",
    "3. Convert to numerals: map words to unique numbers (indices)\n",
    "    - Before: `{\"Pytorch: 0, \"seems\": 1, \"really\": 2, ...}`\n",
    "    - After: `[0, 1, 2, ...]`\n",
    "4. Embedding look-up: map sentences (indices now) to fixed matrices\n",
    "    - ```[[0.1, 0.4, 0.3],\n",
    "       [0.8, 0.1, 0.5],\n",
    "       ...]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting style\n",
    "plt.style.use(('dark_background', 'bmh'))\n",
    "plt.rc('axes', facecolor='none')\n",
    "plt.rc('figure', figsize=(16, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torch\n",
    "\n",
    "# Create instances of fields\n",
    "# The important field here is fix_length: all examples using this field will be padded to, or None for flexible sequence lengths\n",
    "# We are fixing this because we will be using a FNN not an LSTM/RNN/GRU where we can go through uneven sequence lengths\n",
    "max_len = 400\n",
    "text = data.Field(sequential=True, fix_length=max_len, batch_first=True, lower=True, dtype=torch.long)\n",
    "label = data.LabelField(sequential=False, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling splits() class method of datasets.IMDB to return a torchtext.data.Dataset object\n",
    "from torchtext import datasets\n",
    "ds_train, ds_test = datasets.IMDB.splits(text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  25000\n",
      "test :  25000\n",
      "train.fields : {'label': <torchtext.data.field.LabelField object at 0x7f8cbc8c19b0>, 'text': <torchtext.data.field.Field object at 0x7f8cbc8c1978>}\n"
     ]
    }
   ],
   "source": [
    "# Training and test set each 25k samples\n",
    "# 2 fields due to the way we split above\n",
    "print('train : ', len(ds_train))\n",
    "print('test : ', len(ds_test))\n",
    "print('train.fields :', ds_train.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation set\n",
    "import random\n",
    "seed_num = 1337\n",
    "ds_train, ds_valid = ds_train.split(random_state=random.seed(seed_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  17500\n",
      "valid :  7500\n",
      "valid :  25000\n"
     ]
    }
   ],
   "source": [
    "# Now we've training, validation and test set\n",
    "print('train : ', len(ds_train))\n",
    "print('valid : ', len(ds_valid))\n",
    "print('valid : ', len(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "# num_words = 25000\n",
    "num_words = 4000\n",
    "text.build_vocab(ds_train, max_size=num_words)\n",
    "label.build_vocab(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4002\n",
      "Label size: 2\n"
     ]
    }
   ],
   "source": [
    "# Print vocab size\n",
    "print('Vocabulary size: {}'.format(len(text.vocab)))\n",
    "print('Label size: {}'.format(len(label.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 225938), ('a', 112571), ('and', 111513), ('of', 101389), ('to', 94175), ('is', 72550), ('in', 63886), ('i', 49428), ('this', 49096), ('that', 46809)]\n"
     ]
    }
   ],
   "source": [
    "# Print most common vocabulary text\n",
    "most_common_samples = 10\n",
    "print(text.vocab.freqs.most_common(most_common_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 8835), ('pos', 8665)]\n"
     ]
    }
   ],
   "source": [
    "# Print most common labels\n",
    "print(label.vocab.freqs.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 0 label\n",
    "ds_train[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'of',\n",
       " 'all',\n",
       " 'i',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'i',\n",
       " 'love',\n",
       " 'this',\n",
       " 'show!!!',\n",
       " 'but',\n",
       " 'this',\n",
       " 'episode...this',\n",
       " 'episode',\n",
       " 'makes',\n",
       " 'a',\n",
       " 'mockery',\n",
       " 'of',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'show.<br',\n",
       " '/><br',\n",
       " '/>i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'with',\n",
       " 'this',\n",
       " 'episode',\n",
       " 'but',\n",
       " 'they',\n",
       " 'successfully',\n",
       " 'created',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'episode',\n",
       " 'in',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'series.<br',\n",
       " '/><br',\n",
       " '/>there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'story',\n",
       " 'line,',\n",
       " 'everything',\n",
       " 'is',\n",
       " 'chaotic',\n",
       " 'and',\n",
       " 'the',\n",
       " 'jokes.....are',\n",
       " 'crap.<br',\n",
       " '/><br',\n",
       " '/>the',\n",
       " 'way',\n",
       " 'they',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'answer',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'remaining',\n",
       " 'questions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'game.....',\n",
       " 'for',\n",
       " 'example',\n",
       " '\"how',\n",
       " 'do',\n",
       " 'the',\n",
       " 'furlings',\n",
       " 'look',\n",
       " 'like\"',\n",
       " 'by',\n",
       " 'creating',\n",
       " 'that',\n",
       " 'stupid',\n",
       " '\"previously',\n",
       " 'on...\"......is',\n",
       " 'simply',\n",
       " 'embarrassing.<br',\n",
       " '/><br',\n",
       " '/>its',\n",
       " 'clear',\n",
       " 'that',\n",
       " 'the',\n",
       " 'writers',\n",
       " 'are',\n",
       " 'running',\n",
       " 'out',\n",
       " 'of',\n",
       " 'ideas',\n",
       " 'and',\n",
       " 'that',\n",
       " 'is',\n",
       " 'really',\n",
       " 'too',\n",
       " 'bad.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 0 text: broken down into individual portions\n",
    "ds_train[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first of all i just want to say that i love this show!!! but this episode...this episode makes a mockery of the entire show.<br /><br />i don't know what they tried to achieve with this episode but they successfully created the worst episode in the entire series.<br /><br />there is no story line, everything is chaotic and the jokes.....are crap.<br /><br />the way they tried to answer some of the remaining questions in the game..... for example \"how do the furlings look like\" by creating that stupid \"previously on...\"......is simply embarrassing.<br /><br />its clear that the writers are running out of ideas and that is really too bad.\n"
     ]
    }
   ],
   "source": [
    "# Sample 0 text: human readeable sample\n",
    "def show_text(sample):\n",
    "    print(' '.join(word for word in sample))\n",
    "    \n",
    "show_text(ds_train[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and iterable object for our training, validation and testing datasets\n",
    "# Batches examples of similar lengths together that minimizes amount of padding needed\n",
    "batch_size = 10  # Change batch size from 1 to bigger number once explanation is done\n",
    "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if iterator above is an iterable which should show True\n",
    "import collections\n",
    "isinstance(train_loader, collections.Iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 10]\n",
       "\t[.label]:[torch.FloatTensor of size 10]\n",
       "\t[.text]:[torch.LongTensor of size 10x400]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's inside this iteratable object? Our text and label although now everything is in machine format (not \"words\") but in numbers!\n",
    "# The text we saw above becomes a matrix of size 1 x 80 represented by the fixed length we defined before that\n",
    "list(train_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 10]\n",
       "\t[.label]:[torch.FloatTensor of size 10]\n",
       "\t[.text]:[torch.LongTensor of size 10x400]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative to above, this is much faster but the above code is easy to understand and implement\n",
    "next(train_loader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(train_loader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'text'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What methods can we call on this batch object? Text and label\n",
    "test_batch.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    35,     0,  ...,     1,     1,     1],\n",
       "        [  431,  1676,    61,  ...,     1,     1,     1],\n",
       "        [    0,     9,  2922,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [  298,  1120,     0,  ...,   151,   956,     6],\n",
       "        [   10,    20,  3758,  ...,     1,     1,     1],\n",
       "        [    0,     0,    41,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's break this down to check what's in a batch\n",
    "test_batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 400])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 comment per batch, each comment is limited to a size of 80 as we've defined\n",
    "test_batch.text.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extremely weird problem in torchtext where BucketIterator returns a Batch object versus just a simple tuple of tensors containing our text index and labels\n",
    "# So let's fix this with a new class FixBatchGenerator\n",
    "\n",
    "class FixBatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_field):\n",
    "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_field)\n",
    "            yield (X,y)\n",
    "            \n",
    "train_loader, valid_loader, test_loader = FixBatchGenerator(train_loader, 'text', 'label'), FixBatchGenerator(valid_loader, 'text', 'label'), FixBatchGenerator(test_loader, 'text', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1558,    36,   428,  ...,     1,     1,     1],\n",
      "        [  194,   122,    45,  ...,     1,     1,     1],\n",
      "        [    9,   657,     6,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   369,  3496,  ...,     1,     1,     1],\n",
      "        [   44,   346,  2315,  ...,     1,     1,     1],\n",
      "        [    9,  1492,    10,  ...,     1,     1,     1]])\n",
      "tensor([ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.])\n"
     ]
    }
   ],
   "source": [
    "# Text index\n",
    "print(next(train_loader.__iter__())[0])\n",
    "\n",
    "# Text label\n",
    "print(next(train_loader.__iter__())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "   \n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(embedding_dim*embedding_dim, hidden_dim) \n",
    "\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('input layer?', x.requires_grad)\n",
    "        print('embedding layer?', self.embedding.weight.requires_grad)\n",
    "        print('fc1 layer?', self.fc1.weight.requires_grad)\n",
    "        print('fc2 layer?', self.fc2.weight.requires_grad)\n",
    "        print('-'*50)\n",
    "        print('input layer?', x.grad_fn)\n",
    "        print('embedding layer?', self.embedding.weight.grad_fn)\n",
    "        print('fc1 layer?', self.fc1.weight.grad_fn)\n",
    "        print('fc2 layer?', self.fc2.weight.grad_fn)\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.view(-1, embedding_dim*embedding_dim)\n",
    "\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(embedded)\n",
    "\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = F.sigmoid(out)\n",
    "\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(max_len)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = num_words + 1\n",
    "embedding_dim = max_len\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "# learning_rate = 0.1\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Number of groups of parameters\n",
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4001, 400])\n",
      "torch.Size([16, 160000])\n",
      "torch.Size([16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Print parameters\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input layer? True\n",
      "embedding layer? True\n",
      "fc1 layer? True\n",
      "fc2 layer? True\n",
      "--------------------------------------------------\n",
      "input layer? None\n",
      "embedding layer? None\n",
      "fc1 layer? None\n",
      "fc2 layer? None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No grad accumulator for a saved leaf!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-c792fd7b137a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Getting gradients w.r.t. parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Updating parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amn/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amn/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No grad accumulator for a saved leaf!"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        # Training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Load samples\n",
    "        samples = samples.view(-1, max_len).requires_grad_()\n",
    "        labels = labels.view(-1, 1)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(samples)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Testing mode\n",
    "            model.eval()\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for samples, labels in test_loader:\n",
    "                # Load samples\n",
    "                samples = samples.view(-1, max_len)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(samples)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
